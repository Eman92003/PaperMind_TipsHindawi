{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-23T19:03:23.752360Z",
     "iopub.status.busy": "2026-01-23T19:03:23.752189Z",
     "iopub.status.idle": "2026-01-23T19:03:43.190865Z",
     "shell.execute_reply": "2026-01-23T19:03:43.190186Z",
     "shell.execute_reply.started": "2026-01-23T19:03:23.752342Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (0.3.79)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting pypdf2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
      "Collecting transformers==4.52.4\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (26.0rc2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (2.32.5)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.52.4)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (4.67.1)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.37)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
      "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
      "Collecting packaging>=20.0 (from transformers==4.52.4)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.4) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.4) (1.2.1rc0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.52.4) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.52.4) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.52.4) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.52.4) (2026.1.4)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.12.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf2, pyngrok, packaging, faiss-cpu, tokenizers, transformers, langchain-community\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 26.0rc2\n",
      "    Uninstalling packaging-26.0rc2:\n",
      "      Successfully uninstalled packaging-26.0rc2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.1\n",
      "    Uninstalling tokenizers-0.22.1:\n",
      "      Successfully uninstalled tokenizers-0.22.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.1\n",
      "    Uninstalling transformers-4.57.1:\n",
      "      Successfully uninstalled transformers-4.57.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\n",
      "bigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\n",
      "google-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\n",
      "bigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "fastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\n",
      "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed faiss-cpu-1.13.2 langchain-community-0.3.31 packaging-25.0 pyngrok-7.5.0 pypdf2-3.0.1 tokenizers-0.21.4 transformers-4.52.4\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain-core faiss-cpu pypdf2 pyngrok sentence-transformers transformers==4.52.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T19:45:28.972507Z",
     "iopub.status.busy": "2026-01-23T19:45:28.972196Z",
     "iopub.status.idle": "2026-01-23T19:45:28.978242Z",
     "shell.execute_reply": "2026-01-23T19:45:28.977502Z",
     "shell.execute_reply.started": "2026-01-23T19:45:28.972479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Any\n",
    "from langchain.chains import LLMChain\n",
    "import torch\n",
    "import re\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from PyPDF2 import PdfReader\n",
    "import io\n",
    "from fastapi import FastAPI, Request, HTTPException\n",
    "import uvicorn, threading, time, socket\n",
    "from pyngrok import ngrok, conf\n",
    "import base64\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models from Hagging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T19:04:57.995210Z",
     "iopub.status.busy": "2026-01-23T19:04:57.994267Z",
     "iopub.status.idle": "2026-01-23T19:04:57.999018Z",
     "shell.execute_reply": "2026-01-23T19:04:57.998225Z",
     "shell.execute_reply.started": "2026-01-23T19:04:57.995178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "QA_model = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T19:04:59.392849Z",
     "iopub.status.busy": "2026-01-23T19:04:59.392517Z",
     "iopub.status.idle": "2026-01-23T19:09:28.593362Z",
     "shell.execute_reply": "2026-01-23T19:09:28.592805Z",
     "shell.execute_reply.started": "2026-01-23T19:04:59.392820Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5553224c06e148c3a4c62e4272dc6f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931bc4b378cf4c67afeae25a84d54a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24304d7df3d4253a167c58807b5e516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6893b1e01947405b8ef21661bd23948e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/622 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f395fc69b52499cbcf0ea9958f4bfe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e889738187e046cd97c28247ef9c2254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfdb7c0ff5fa4e218778dfb95683238d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f0d44394944ada8e3aae51f0c1e65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3658ea9def4c450eb9fd28501edae526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88117b6f6254c789338135740967833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcfec86f1b74826bd1951450495cc86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28949aaae0ee4ad2aa559cc9ff266859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2faf66865f934ae4ae0908c2a2167cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_QA = AutoTokenizer.from_pretrained(QA_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(QA_model, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading PDF using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T19:09:53.015801Z",
     "iopub.status.busy": "2026-01-23T19:09:53.015463Z",
     "iopub.status.idle": "2026-01-23T19:09:53.019305Z",
     "shell.execute_reply": "2026-01-23T19:09:53.018704Z",
     "shell.execute_reply.started": "2026-01-23T19:09:53.015771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pdf_path = \"/kaggle/input/scientific-paper/Plant_Disease_Classification.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T19:09:46.966610Z",
     "iopub.status.busy": "2026-01-23T19:09:46.966023Z",
     "iopub.status.idle": "2026-01-23T19:09:46.970931Z",
     "shell.execute_reply": "2026-01-23T19:09:46.970371Z",
     "shell.execute_reply.started": "2026-01-23T19:09:46.966583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_pdf(path):\n",
    "    '''this function aims to load, extract, split, embed and create vector database using langchain tools'''\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"])\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    embedding = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "    vectordb = FAISS.from_documents(chunks, embedding)\n",
    "    return documents, chunks, embedding, vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T19:46:22.495840Z",
     "iopub.status.busy": "2026-01-23T19:46:22.495451Z",
     "iopub.status.idle": "2026-01-23T19:46:22.500585Z",
     "shell.execute_reply": "2026-01-23T19:46:22.499953Z",
     "shell.execute_reply.started": "2026-01-23T19:46:22.495811Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_pdf(text):\n",
    "    '''this function aims to split, embed and create vector database using langchain tools used in deployment'''\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"])\n",
    "    doc = Document(page_content=text)\n",
    "    ##convert the output text into document to split it using lamgchin splitter\n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "    embedding = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "    vectordb = FAISS.from_documents(chunks, embedding)\n",
    "    return chunks, embedding, vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-23T19:09:55.476175Z",
     "iopub.status.busy": "2026-01-23T19:09:55.475434Z",
     "iopub.status.idle": "2026-01-23T19:10:11.338463Z",
     "shell.execute_reply": "2026-01-23T19:10:11.337702Z",
     "shell.execute_reply.started": "2026-01-23T19:09:55.476143Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/2586350219.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=embedding_model)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce0400eeed84da69bcd0ae262b45b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718a7a4180894f428cde9db6bdac5cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acbb2d1c3dcc45d0b7d975379df41d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a647f282494ceda82a8fa4bab3d0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766e936b62da4c469f6b0ff287099471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221d3755c1aa44ffb0834ab412fa48cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6aca115c33c41ccab6d120ccc41908b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b013f53769e4e8ea21bfbec24dbdfd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e317ffb915fe4e188a6c174c7a640a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54646d752b3246c7987a898b8c0eeebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9751c7fc202a434d9f252d942c6d5066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents, chunks, embedding, vectordb = extract_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-23T19:10:25.158952Z",
     "iopub.status.busy": "2026-01-23T19:10:25.158587Z",
     "iopub.status.idle": "2026-01-23T19:10:25.166403Z",
     "shell.execute_reply": "2026-01-23T19:10:25.165673Z",
     "shell.execute_reply.started": "2026-01-23T19:10:25.158921Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received 15 September 2023, accepted 4 October 2023, date of publication 16 October 2023, date of current version 20 October 2023.\n",
      "Digital Object Identifier 10.1 109/ACCESS.2023.3324722\n",
      "Machine Learning and Deep Learning for Plant\n",
      "Disease Classification and Detection\n",
      "VASILEIOS BALAFAS\n",
      " , EMMANOUIL KARANTOUMANIS\n",
      " ,\n",
      "MALAMATI LOUTA\n",
      " , (Senior Member, IEEE),\n",
      "AND NIKOLAOS PLOSKAS\n",
      "Department of Electrical and Computer Engineering, University of Western Macedonia, 50100 Kozani, Greece\n",
      "Corresponding author: Nikolaos Ploskas (nploskas@uowm.gr)\n",
      "This work was supported by the European Regional Development Fund of the European Union and Greek national funds through the\n",
      "Operational Program Competitiveness, Entrepreneurship and Innovation under Project MIS 5047196. The publication of the article in OA\n",
      "mode was financially supported by HEAL-Link.\n",
      "ABSTRACT Precision agriculture is a rapidly developing field aimed at addressing current concerns\n",
      "about agricultural sustainability. Machine learning is the cutting edge technology underpinning precision\n",
      "agriculture, enabling the development of advanced disease detection and classification methods. This\n",
      "paper presents a review of the application of machine learning and deep learning techniques in precision\n",
      "agriculture, specifically for detecting and classifying plant diseases. We propose a novel classification\n",
      "scheme that categorizes all relevant works in the associated classes. We separate the studies into two main\n",
      "categories depending on the methodology that they use (i.e., classification or object detection). In addition,\n",
      "we present the available datasets for plant disease detection and classification. Finally, we perform an\n",
      "extensive computational study on five state-of-the-art object detection algorithms on PlantDoc dataset to\n",
      "detect diseases present on the leaves, and eighteen state-of-the-art classification algorithms on PlantDoc\n",
      "dataset to predict whether or not there is a disease in a leaf. Computational results show that object detection\n",
      "accuracy is high with YOLOv5. For the image classification task, the networks ResNet50 and MobileNetv2\n",
      "have the most optimal trade-off on accuracy and training time.\n",
      "INDEX TERMS Classification, deep learning, disease detection, machine learning, object detection,\n",
      "precision agriculture.\n",
      "I. INTRODUCTION\n",
      "Any nation’s economic development depends significantly\n",
      "on agriculture. Meeting the population’s current food needs\n",
      "has become a difficult problem because of the growing pop-\n",
      "ulation, frequent changes in weather, and limited resources.\n",
      "On top of the aforementioned challenges, crop diseases have\n",
      "been increasing in severity and scale. Crop diseases cause\n",
      "production losses that can be mitigated with continuous\n",
      "monitoring. Researchers from the Food and Agriculture\n",
      "Organization of the United Nations predicted that plant\n",
      "diseases alone cost the global economy about US$220 billion\n",
      "annually [1]. Developing new methods to detect diseases on\n",
      "plants or leaves at an early stage can significantly increase\n",
      "yield potential.\n",
      "The associate editor coordinating the review of this manuscript and\n",
      "approving it for publication was Pasquale De Meo\n",
      ".\n",
      "Precision agriculture is a rapidly developing field aimed at\n",
      "addressing current concerns about agricultural sustainability.\n",
      "Machine learning (ML) is the cutting edge technology\n",
      "underpinning precision agriculture, allowing the machine\n",
      "to learn without having to be programmed directly, and\n",
      "in conjunction with Internet of Things (IoT) enabled farm\n",
      "equipment, is the future of agriculture. There are several\n",
      "studies that have employed or proposed ML methods to detect\n",
      "or classify plant diseases. The majority of these works take as\n",
      "input a plant/leaf image and detect whether or not there is a\n",
      "disease. These works treat the problem as a classification one,\n",
      "either binary classification (healthy or diseased plant/leaf)\n",
      "or multi-class classification (where various diseases are\n",
      "targeted). Classical ML methods, like Random Forest (RF)\n",
      "and Deep Learning (DL) ones, have been utilized for this\n",
      "purpose. On the other hand, there are fewer works that aim to\n",
      "detect both the type of the disease and the diseased regions\n",
      "114352\n",
      "\n",
      " 2023 The Authors. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\n",
      "For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME 11, 2023\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "of the input image, i.e., treating the problem as an object\n",
      "detection one. The latter problem is more important than\n",
      "the classification problem in cases where either there exist\n",
      "multiple plant diseases in the input image or we want to\n",
      "know the exact region of the diseased plants in an image\n",
      "that covers a large part of the crop (such images are taken\n",
      "by Unmanned Aerial V ehicles (UA Vs)). Moreover, the object\n",
      "detection problem is more difficult than the classification\n",
      "problem and the DL methods used to detect objects do not\n",
      "have good results in uncontrolled places, i.e., in images where\n",
      "the objects are placed in a noisy environment.\n",
      "In this paper, we aim to review ML and DL methods\n",
      "that have been applied to either classify or detect plant\n",
      "diseases. V arious papers reviewed ML and DL methods\n",
      "in precision agriculture. Below, we present those papers\n",
      "that include works focused on plant disease detection. For\n",
      "example, Mekonnen et al. [2] provided a study of how\n",
      "different ML techniques are used in sensor data analytics in\n",
      "the agricultural sector. The authors highlighted the benefits of\n",
      "Artificial Intelligence (AI) in agriculture and emphasized that\n",
      "IoT and AI are increasingly becoming essential components\n",
      "for enhancing agricultural productivity and efficiency. They\n",
      "concluded that although clustering is the most common\n",
      "learning model for such problems, there is no general\n",
      "prescription for selecting algorithms, and most of the time,\n",
      "it is a trial and error process in doing so. Finally, they\n",
      "examined some of the ML methods used in agriculture such\n",
      "as regression algorithms, Decision Trees (DT), ensemble\n",
      "learning, Bayesian models, Support V ector Machines (SVM),\n",
      "and Artificial Neural Networks (ANNs).\n",
      "Benos et al. [3] presented a review of various works\n",
      "aimed at exploring ML in agriculture. Initially, they described\n",
      "the four basic categories in which ML algorithms are\n",
      "involved. Crop management is subdivided into five categories\n",
      "(yield prediction, disease detection, weed detection, crop\n",
      "recognition, and crop quality), water management, soil\n",
      "management, and livestock management. They performed\n",
      "an extensive search on various search engines and selected\n",
      "articles for the period from 2018 to 2020. Finally, they\n",
      "concluded that: (i) a large percentage of research articles were\n",
      "about crop management, (ii) ANNs were the most effective\n",
      "ML models, (iii) the most investigated crops in the papers\n",
      "were maize, wheat, rice, and soybean, and finally, (iv) the\n",
      "most utilized input data was mainly RGB images and then\n",
      "weather, soil, water, and crop quality.\n",
      "Li et al. [4] presented a review of the research progress\n",
      "of DL methods for crop leaf disease identification. Initially,\n",
      "they reviewed the relevant metrics, the datasets used in\n",
      "the computational experiments, and the data enhancement\n",
      "methods. Then, they presented works based on DL methods\n",
      "in order to predict diseases in crop leaves. Finally, they\n",
      "concluded that: (i) most DL frameworks are not robust,\n",
      "achieving good results only on specific datasets, (ii) most\n",
      "works use synthetic images generated in the laboratory, and\n",
      "(iii) studies focused on early detection of disease are limited.\n",
      "FIGURE 1. Publications per year for papers including the words ‘‘plant’’ ,\n",
      "‘‘disease’’ , ‘‘detection’’ , and ‘‘machine learning’’ either on their title or\n",
      "abstract or keywords (source: Scopus).\n",
      "Liu and Wang [5] reviewed works focusing on methods\n",
      "for detecting plant diseases and pests published during the\n",
      "period from 2014 to 2020. They presented various methods\n",
      "proposed in the literature and discussed their advantages and\n",
      "disadvantages. They also presented the publicly available\n",
      "datasets for plant diseases and pest detection methods.\n",
      "Finally, they concluded that: (i) the majority of research\n",
      "findings are limited to the laboratory setting and are solely\n",
      "relevant to plant disease and pest images obtained during that\n",
      "period, (ii) early diagnosis is very difficult, and thus, only a\n",
      "few works deal with this, and (iii) less time- and memory-\n",
      "consuming DL models are in need for real-time prediction of\n",
      "diseases and pests.\n",
      "Y uan et al. [6] examined the progress in technologies\n",
      "utilized for agricultural disease image recognition such as DL\n",
      "and transfer learning. They also analyzed key challenges that\n",
      "require attention to progress research in this field, including\n",
      "creating image datasets, choosing big data auxiliary domains,\n",
      "and improving transfer learning techniques. It is essential\n",
      "to create image datasets captured under actual production\n",
      "conditions to develop a feasible image recognition system for\n",
      "agricultural diseases.\n",
      "Although there are numerous review papers discussing ML\n",
      "and DL techniques used in precision agriculture, there are\n",
      "only a few review papers that focus on plant disease detection\n",
      "methods, and none of them discusses the two main techniques\n",
      "used in the literature, classification and object detection\n",
      "methods. Typically, the review papers devote a section for\n",
      "plant disease detection methods. In addition, all reviews\n",
      "present works published until 2020. As shown in Figure 1,\n",
      "the publications that deal with ML and DL methods for plant\n",
      "disease detection have doubled since 2020. Therefore, there\n",
      "is a need for a comprehensive review on ML and DL methods\n",
      "for plant disease detection and classification.\n",
      "In addition, computational comparisons of algorithms used\n",
      "for plant disease detection and classification are missing\n",
      "in the literature. Most papers in this area compare their\n",
      "proposed algorithm with a single state-of-the-art method.\n",
      "In this work, we present a comparison of five state-of-the-\n",
      "art object detection algorithms for identifying diseases on\n",
      "VOLUME 11, 2023 114353\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "plants and eighteen state-of-the-art classification algorithms\n",
      "for plant disease classification on a widely used dataset.\n",
      "Our contributions can be summarized as follows:\n",
      "• We present a detailed review of ML and DL algorithms\n",
      "for plant disease detection and classification. Most\n",
      "studies review works until 2020 and we also include\n",
      "recent works (published until June 2022). We distinguish\n",
      "these works based on the goal of the methods used, i.e.,\n",
      "whether classification or object detection is targeted.\n",
      "We present a thorough review on 65 works for classi-\n",
      "fication and 14 works for object detection.\n",
      "• We propose a novel classification scheme that cate-\n",
      "gorizes all relevant works in the associated classes.\n",
      "By classifying relevant papers according to these\n",
      "classes, we can easily provide an overview of different\n",
      "techniques used in the field.\n",
      "• We provide a summary of the existing datasets that are\n",
      "being used for plant disease detection and classification\n",
      "tasks. We include details about the types of data they\n",
      "contain, as well as the classes and labels available within\n",
      "these datasets.\n",
      "• We perform extensive computational analysis on five\n",
      "state-of-the-art object detection algorithms for plant\n",
      "disease detection and eighteen state-of-the-art classifi-\n",
      "cation algorithms for plant disease classification on a\n",
      "widely used dataset. This is the first time that such a\n",
      "systematic study has been carried out.\n",
      "• We discuss gaps in the existing literature and future\n",
      "directions for this field.\n",
      "The structure of the paper is as follows. Section II reviews\n",
      "research works that proposed/utilized ML and DL algorithms\n",
      "for predicting/recognizing plant diseases. In this section,\n",
      "we separate the papers according to whether they used\n",
      "classification or object detection techniques. In Section III,\n",
      "we propose a classification scheme, categorize all relevant\n",
      "works in the associated classes and present statistics of\n",
      "the most common methods/algorithms/datasets/metrics used\n",
      "by the reviewed papers. Section IV provides an overview\n",
      "of the datasets that are commonly used in the literature\n",
      "for plant disease detection and classification using both\n",
      "object detection and classification techniques. Section V\n",
      "includes the computational comparison of five state-of-the-\n",
      "art object detection algorithms for plant disease detection\n",
      "and eighteen state-of-the-art classification algorithms for\n",
      "plant disease classification. In Section VI, we highlight the\n",
      "practical implication of our findings. Section VII includes the\n",
      "open challenges that need to be addressed, while Section VIII\n",
      "includes the future directions for addressing these open\n",
      "challenges. Finally, Section IX summarizes the work of this\n",
      "paper.\n",
      "II. LITERATURE REVIEW\n",
      "This section provides a detailed literature review of disease\n",
      "detection techniques. All papers have been published prior\n",
      "to October 2022. To identify relevant studies on ML and\n",
      "DL methods for plant disease detection and classification,\n",
      "we conducted a search using various search engines,\n",
      "including Scopus, ScienceDirect, Scholar, and Web of Sci-\n",
      "ence. Keywords like ‘‘machine learning’’, ‘‘deep learning’’,\n",
      "‘‘classification’’, ‘‘disease detection’’, ‘‘healthy plant’’, and\n",
      "‘‘diseased plant’’, were used. Once a relevant work was\n",
      "identified, its references were scanned in order to find\n",
      "relevant works that were not found in the initial search\n",
      "procedure. The abstract of each paper was reviewed in order\n",
      "for all co-authors to decide its appropriateness and inclusion\n",
      "in the paper. All papers that did not have a focus on the ML\n",
      "algorithms were excluded. For example, there exist papers\n",
      "that refer to plant disease detection using ML methods but\n",
      "their main focus is the data gathering part, e.g., through UA Vs\n",
      "or Unmanned Ground V ehicles (UGVs). In addition, non-\n",
      "English texts, technical reports, review papers, and Master’s\n",
      "and Doctoral theses were excluded. Overall, 79 papers (65 for\n",
      "classification and 14 for object detection) were discovered.\n",
      "A. CLASSIFICATION\n",
      "Mohanty et al. [7] trained AlexNet [8] and GoogLeNet\n",
      "[9] for detecting 26 diseases on 14 crop species using\n",
      "the PlantVillage dataset [10] with 54, 306 images. In the\n",
      "preprocessing step, they downsized images to 256 × 256.\n",
      "They also experimented with the original PlantVillage\n",
      "dataset, with a gray-scaled version of this dataset, and a\n",
      "version in which the leaves were segmented. The models\n",
      "were trained from scratch as well as by utilizing pre-trained\n",
      "models on the ImageNet dataset through transfer learning.\n",
      "The authors reported an accuracy of 99.35% and 85.53%\n",
      "using the GoogLeNet and AlexNet models, respectively.\n",
      "A plant disease detection approach based on Convolutional\n",
      "Neural Network (CNN) using the Caffe DL framework [11]\n",
      "was proposed by Sladojevic et al. [12]. They collected images\n",
      "from various sources and they used data augmentation tech-\n",
      "niques (affine transformation, perspective transformation,\n",
      "and rotation) to generate more images. In the preprocessing\n",
      "step, they downsized images to 256 × 256 and removed the\n",
      "duplicates. In addition, they did not consider images with\n",
      "smaller resolution than 500px. They modified the CaffeNet\n",
      "architecture by altering the last layer and the output of\n",
      "the softmax layer. They achieved an accuracy of 96.3% on\n",
      "average.\n",
      "In their work, Amara et al. [13] applied the LeNet archi-\n",
      "tecture [14] to classify diseases in banana leaves, belonging\n",
      "to three different categories. In the preprocessing step, they\n",
      "downsized images to 60×60 and converted to grayscale. They\n",
      "tested the proposed architecture on the PlantVillage dataset,\n",
      "achieving an accuracy between 92% and 99%.\n",
      "Brahimi et al. [15] used two CNN models, AlexNet\n",
      "and GoogleNet, on the PlantVillage dataset for predicting\n",
      "diseases on tomato leaves. They pre-trained the models on\n",
      "ImageNet and then fine-tuned them by replacing the output\n",
      "layer with a new one having nine classes, as the number of the\n",
      "diseases that they consider in this study. In the preprocessing\n",
      "step, they downsized images to 256 × 256. GoogleNet\n",
      "114354 VOLUME 11, 2023\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "achieved an accuracy from 97.71% to 99.19%, while AlexNet\n",
      "achieved an accuracy from 94.35% to 98.66%.\n",
      "Cruz et al. [16] implemented a CNN network based on the\n",
      "LeNet architecture for detecting olive quick decline symp-\n",
      "toms. They initially trained the network on the PlantVillage\n",
      "dataset, then retrained on a custom dataset. They improved\n",
      "the performance of the network by providing information\n",
      "about the edge patterns and the shape information. In the\n",
      "preprocessing step, they downsized images to 256× 256. The\n",
      "authors reported an accuracy of 99%.\n",
      "DeChant et al. [17] proposed a three-stage approach using\n",
      "CNN models for identifying NLB-infected maize plants.\n",
      "A three-stage approach was utilized in this study by initially\n",
      "training various CNNs to detect lesions in small patches of\n",
      "the images. Then, the CNNs generated heat maps indicating\n",
      "regions in the images that may be affected. Finally, they\n",
      "trained the CNN models using as input the heat maps of\n",
      "the original images. They generated a custom dataset of\n",
      "1, 796 images. Their approach resulted in a classification\n",
      "accuracy of 96.7%.\n",
      "In their study, Liu et al. [18] employed the AlexNet\n",
      "architecture to detect apple leaf diseases, utilizing a dataset of\n",
      "13, 689 images that contained four diseases: mosaic, brown\n",
      "spot, rust, and Alternaria leaf spot. Prior to classification, they\n",
      "performed image preprocessing by rotating and sharpening\n",
      "the images. Their approach yielded an overall accuracy\n",
      "of 97.62\n",
      "Lu et al. [19] introduced a multi-stage CNN architecture\n",
      "that was based on AlexNet, aimed at identifying diseases\n",
      "in rice plants. The authors collected images for their study\n",
      "from an agricultural pest and insect pests database, as well as\n",
      "from a book that contained images of diseased plants. They\n",
      "preprocessed the images by resizing them to 512 × 512 pixels\n",
      "and then applied the ZCA-Whitening technique to remove\n",
      "the correlation between data. The accuracy of the proposed\n",
      "model was 95.48%.\n",
      "Oppenheim and Shani [20] used a CNN based on VGG\n",
      "[21] to classify potatoes into five classes, four diseased ones\n",
      "and a healthy potato class. They acquired 400 images of\n",
      "contaminated potatoes using three simple digital cameras and\n",
      "augmented data by flipping and cropping the images. In the\n",
      "preprocessing step, they downsized images to 224 × 224 and\n",
      "converted to grayscale. They experimented with different\n",
      "ratios of train-test sets and concluded that the CNN can\n",
      "achieve an accuracy between 83% for the model trained with\n",
      "only 10% of the data to 96% for the model trained with 90%\n",
      "of the data.\n",
      "Wang et al. [22] trained and fine-tuned four CNN models,\n",
      "i.e., VGG16 [21], VGG19 [21], InceptionV3 [23], and\n",
      "ResNet50 [24], for detecting apple leaf black rot. The authors\n",
      "utilized the PlantVillage dataset, which contained two classes\n",
      "of images: healthy apple leaves and apple leaf black rot.\n",
      "The images were further categorized by botanists into four\n",
      "stages: healthy, early stage, middle stage, and end-stage.\n",
      "In the preprocessing step, they downsized images to various\n",
      "ratios according to the CNN that they used. They evaluated\n",
      "several models, and the VGG16 model produced the best\n",
      "results, achieving an accuracy of 90.4%.\n",
      "Abdulridha et al. [25] demonstrated a non-destructive\n",
      "remote sensing approach for detecting the Laurel Wilt disease\n",
      "in infected avocado trees. They used a portable spectral\n",
      "data collection system to classify the data into healthy or\n",
      "not based on causes that elicit comparable symptoms, such\n",
      "as iron and nitrogen deficiency. Two sets of images with\n",
      "spectral resolutions of 10nm and 40nm were gathered in order\n",
      "to feed the DT and MultiLayer Perceptron (MLP) neural\n",
      "network model. They concluded that MLP has a near-perfect\n",
      "classification accuracy (100% at the early stage and 91% at\n",
      "the late stage) compared to DT (82% at the early stage and\n",
      "82% at the late stage).\n",
      "Barbedo [26] investigated the major factors influencing\n",
      "the design and performance of CNNs for plant disease\n",
      "recognition. To anticipate corn diseases, they used the PDDB\n",
      "dataset [27] that includes 50,000 images and 171 diseases.\n",
      "They also studied nine factors that influence disease detection\n",
      "in maize fields. They trained the model with four different\n",
      "datasets and the best accuracy was 87% with a subdivided\n",
      "dataset.\n",
      "Ferentinos [28] employed simple leaf images to perform\n",
      "plant disease detection and analysis using CNN models.\n",
      "They used the PlantVillage dataset with 87,848 images. This\n",
      "dataset has 25 plants and 58 distinct classes. Furthermore,\n",
      "five different CNN algorithms were used: AlexNet, AlexNe-\n",
      "tOWTBn [29], GoogLeNet, Overfeat [30], and VGG. In the\n",
      "preprocessing step, they downsized images to 256 × 256.\n",
      "The best results were obtained by the VGG algorithm with\n",
      "99.48% success rate at the original image dataset and 98.87%\n",
      "success rate at the preprocessed image dataset.\n",
      "Lu et al. [31] studied the possibility of detecting multiple\n",
      "diseases on tomato leaves at different stages using a portable\n",
      "high-resolution spectral sensor. They utilized the K-Nearest\n",
      "Neighbor (KNN) algorithm to classify the sensor data into\n",
      "four categories: healthy, asymptomatic, early stage, and\n",
      "late stage. Principal Component Analysis (PCA) was used\n",
      "to analyze 57 spectral vegetation indices. They used six\n",
      "principal components and two spectral vegetation indices\n",
      "compositions. The healthy leaves had an accuracy between\n",
      "85.7% and 100%, the asymptomatic leaves between 86.4%\n",
      "and 100%, the early stage leaves between 73.5% and\n",
      "93.0%, and the late stage leaves between 77.1% and 100%,\n",
      "depending on the principal component.\n",
      "In their study, Fuentes et al. [32] proposed a diagnosing\n",
      "system based on a refinement filter bank for detecting dis-\n",
      "eases and pests on tomato plants. They aimed to overcome the\n",
      "issues of class imbalance and false positives by constructing a\n",
      "refinement filter bank framework. The primary diagnosis unit\n",
      "was the bounding box generator, which was used to create\n",
      "bounding boxes containing the diseased area’s position. The\n",
      "secondary diagnosis unit was the CNN filter bank, which was\n",
      "used to validate the results. The proposed method achieved\n",
      "VOLUME 11, 2023 114355\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "a recognition rate of around 96% on a set of 5, 000 images\n",
      "collected from various tomato farms in South Korea.\n",
      "Kerkech et al. [33] presented a method that uses a CNN,\n",
      "namely LeNet, and histograms to forecast diseased areas of\n",
      "grapevines and identify symptoms in vineyards. In order to\n",
      "overcome memory constraints, the streaming technique was\n",
      "utilized in the learning process for loading a small amount of\n",
      "data in each iteration. They collected the data through visible-\n",
      "domain images taken by UA Vs. They conducted experiments\n",
      "by utilizing various color spaces, vegetation indices, and their\n",
      "combination, to detect plant diseases. The approach achieved\n",
      "an accuracy of 95.8\n",
      "Pineda et al. [34] predicted the disease caused by the\n",
      "bacterium Dickeya dadantii in melon leaves. They used three\n",
      "ML algorithms to train models and they used thermography\n",
      "and fluorescence imaging techniques to produce more\n",
      "accurate results. The ML algorithms that they used were\n",
      "Logistic Regression Analysis (LRA), SVM, and ANNs. They\n",
      "found that ANNs had better accuracy than the other models\n",
      "(99.1%) when classifying images as entire leaves.\n",
      "Sharif et al. [35] proposed a two-phase hybrid technique\n",
      "for detecting and categorizing diseases in plants, specifically\n",
      "citrus fruits and leaves. The first phase involved identifying\n",
      "lesion areas, while the second phase focused on classifying\n",
      "the type of disease present. To select the most relevant\n",
      "features, the authors used a hybrid feature selection method\n",
      "that included PCA score, entropy, and a skewness-based\n",
      "covariance vector. This information is provided as input into\n",
      "the MultiClass SVM (M-SVM) for disease classification.\n",
      "They used images from open databases (PlantVillage and\n",
      "Citrus Diseases Database [36]) and a custom-collected image\n",
      "database. The proposed method achieved an accuracy of 97%.\n",
      "Zhang et al. [37] enhanced maize leaf disease recognition\n",
      "accuracy and reduced the number of network parameters.\n",
      "They gathered 500 images from several sources, including\n",
      "the PlantVillage dataset, and divided the data into eight\n",
      "categories for diseased maize leaves and one category for\n",
      "healthy leaves. To increase the sample of images, they rotated\n",
      "the images by 90°, 180°, and 270°. They used Cifar10 [38]\n",
      "and GoogLeNet, changing the base learning rate to impact the\n",
      "network’s identification accuracy. They additionally tweaked\n",
      "hyperparameter settings and enhanced the models. In the\n",
      "preprocessing step, they downsized images to 224 × 224.\n",
      "They achieved an accuracy of 98.9% for the GoogLeNet\n",
      "model and 98.8% for the Cifar10 model.\n",
      "An early disease detection technique for avocado trees to\n",
      "identify Laurel Wilt was proposed by Abdulridha et al. [39].\n",
      "They classified the data into five main categories: Laurel\n",
      "Wilt disease, healthy trees, trees infected by phytophthora\n",
      "root rot, and trees with iron and nitrogen deficiencies.\n",
      "To produce better results, they used image acquisition and\n",
      "preprocessing techniques, and feature extraction techniques.\n",
      "The classification was performed with two classification\n",
      "methods, MLP and KNN. They used a custom dataset\n",
      "acquired through two sensing systems. The best results to\n",
      "predict Laurel Wilt was achieved from the MLP method with\n",
      "an accuracy of 99%.\n",
      "Al-Saddik et al. [40] studied the optimal spectral bands for\n",
      "the development of a multispectral camera used on a UA V\n",
      "for recognizing diseased grapevine fields with the disease\n",
      "Flavescence dorée. The specific disease is a contagious one\n",
      "that is incurable and can lead to severe production loss.\n",
      "For selecting the optimum spectral bands capable of distin-\n",
      "guishing infected from healthy leaves, two spectral analysis\n",
      "methodologies were presented. The first one performs a\n",
      "feature selection strategy, utilizing the successive project\n",
      "algorithm with several spectral preprocessing approaches.\n",
      "The second method looks at a variety of classic vegetation\n",
      "metrics. The two classifiers employed in this research are the\n",
      "SVM and discriminant analysis. The most accurate models\n",
      "were calculated as a function of the grapevine variety in\n",
      "question. The successive projection algorithm approach was\n",
      "better when it came to common vegetation metrics, with an\n",
      "accuracy in classification greater than 96%.\n",
      "A new dataset consisting of a large number of labeled\n",
      "images of leaves captured in real environments was intro-\n",
      "duced by Arsenovic et al. [41]. They also proposed an\n",
      "augmentation technique that utilizes Generative Adversarial\n",
      "Networks (GANs). A two-stage CNN algorithm was then\n",
      "proposed for plant disease detection, and the proposed model\n",
      "achieved an accuracy of 93.67%.\n",
      "The focus of Barbedo [42] was on a distinct issue\n",
      "related to disease detection. They pointed out that current\n",
      "databases lacked the necessary images to accurately represent\n",
      "the various conditions and characteristic symptoms, which\n",
      "could not be reproduced using existing data augmentation\n",
      "techniques. As a result, they looked at using particular lesions\n",
      "and areas for the task, instead of looking at the entire leaf.\n",
      "They managed to expand the existing database because each\n",
      "region has its own characteristics. They created a dataset with\n",
      "images taken from typical cameras, smartphones, or DSLR\n",
      "cameras. In the preprocessing step, they downsized images\n",
      "to 224 × 224 × 3. They achieved 12% higher accuracy\n",
      "than in the original image dataset. Four categories were\n",
      "predicted: healthy leaves with an accuracy of 89%, leaves\n",
      "with mild disease with an accuracy of 31%, leaves with\n",
      "moderate disease with an accuracy of 87%, and leaves with\n",
      "severe disease with an accuracy of 94%.\n",
      "Coulibaly et al. [43] developed a feature extraction-\n",
      "based methodology that was predicated on the CNN model\n",
      "VGG16, which has been pre-trained on ImageNet. They\n",
      "used 124 images of pearl millet from the ImageNet dataset,\n",
      "with mildew diseases and healthy plants. Additionally, a data\n",
      "augmentation technique was utilized to augment the dataset\n",
      "by increasing the number of images. In the preprocessing\n",
      "step, they downsized images to 150 × 150. Transfer learning\n",
      "was employed with feature extraction, resulting in an\n",
      "accuracy of 95%.\n",
      "Dhingra et al. [44] presented methods for recognizing\n",
      "the diseases on the leaves of basil using digital image\n",
      "114356 VOLUME 11, 2023\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "processing techniques. They used nine different classifiers\n",
      "to classify the diseases. The researchers gathered images\n",
      "from an herb garden and standardized the surface condition\n",
      "by removing any non-uniform distribution of dust across\n",
      "the leaves, ensuring consistency across all leaf categories.\n",
      "The nine classifiers used were DT, SVM, linear models,\n",
      "Naives Bayes, KNN, AdaBoost, discriminant analysis, RF,\n",
      "and ANNs. The classification separated the images into\n",
      "two categories, diseased and healthy. The dataset includes\n",
      "200 healthy and 200 diseased leaves. Out of the nine\n",
      "classifiers used, RF achieved the highest accuracy of 98.4%.\n",
      "Hu et al. [45] presented a low-shot learning method to\n",
      "predict diseases in tea leaves. They created their image\n",
      "dataset by using UA Vs. The tea disease spots in the images\n",
      "were separated using the SVM method. Conditional Deep\n",
      "Convolutional Generative Adversarial Networks (CDCGAN)\n",
      "[46] were used to supplement disease spot samples, while\n",
      "VGG16 DL networks are used to identify disease spots.\n",
      "They compared five different methods: SVM, DT, RF, and\n",
      "two different CNN methods, CDCGAN and VGG16, and\n",
      "they concluded that CNN achieved better results. The results\n",
      "obtained for the prediction of tea red scab, tea red leaf spot,\n",
      "and tea leaf blight, reached an average accuracy of 90% for\n",
      "the CNN methods.\n",
      "The goal of Huang et al. [47] was to develop a decision\n",
      "support system for spraying machines that could identify\n",
      "Helminthosporium leaf spots in wheat fields using remote\n",
      "sensing data from a UA V . The dataset was divided into\n",
      "four categories based on disease severity: normal, light,\n",
      "medium, and heavy. In the preprocessing step, they extracted\n",
      "100 × 100 samples from the images. A CNN was used for\n",
      "classification, and the proposed method achieved an accuracy\n",
      "of 91.43%.\n",
      "In their study, Abdulridha et al. [48] aimed to detect\n",
      "two diseases in tomato crops using hyperspectral imaging\n",
      "captured by a UA V in the range of 380–1020nm. The\n",
      "tomato leaves were classified into four categories: healthy,\n",
      "asymptomatic, early, and late disease stages. They generated\n",
      "35 spectral vegetation indices to identify the best indica-\n",
      "tors for disease detection and diagnosis and used MLP\n",
      "and Stepwise Discriminant Analysis (STDA) classification\n",
      "methods. They found that the blue band (408–420nm), red\n",
      "band (630–650 nm), and red edge (730–750 nm) were the\n",
      "most effective wavebands for disease detection. The spectral\n",
      "signatures of the two diseases were significantly different\n",
      "at all stages of disease progression, despite having similar\n",
      "symptoms in the early stages. Both laboratory and field tests\n",
      "showed satisfactory results for detecting both diseases in\n",
      "asymptomatic, early, and late stages. The MLP algorithm\n",
      "outperformed STDA, achieving an accuracy of 97% to 99%\n",
      "for all stages.\n",
      "Abdulridha et al. [49] proposed nondestructive methods\n",
      "to detect diseases that affect tomato crops in two different\n",
      "datasets, a dataset created at the laboratory (benchtop\n",
      "scanning) and a dataset with images captured in the field\n",
      "(using a UA V). These images consisted of images from\n",
      "healthy and diseased plants. The diseases that they targeted\n",
      "were: bacterial spot, target spot, and tomato yellow leaf\n",
      "curl. The diseased plants were identified and classified using\n",
      "several V egetative Indices and the M statistic technique.\n",
      "They utilized two classification methods, STDA and RBF.\n",
      "For all diseases, both in the symptomatic and asymptomatic\n",
      "stages (i.e., before symptoms become obvious to direct visual\n",
      "observations), the classification results showed extremely\n",
      "accurate discrimination between healthy and diseased plants.\n",
      "In the field, the best classification results were achieved for\n",
      "tomato yellow leaf curl, bacterial spot, and target spot with\n",
      "accuracies of 100%, 98%, and 96%, respectively.\n",
      "In their study, Abdulridha et al. [50] aimed to detect\n",
      "Powdery mildew in squash at various stages of disease\n",
      "development, including asymptomatic, early, intermedi-\n",
      "ate, and late stages. They utilized hyperspectral imaging\n",
      "(380–1020nm) and machine learning (ML) algorithms to\n",
      "perform the detection, collecting data both in the lab and in\n",
      "the field using a UA V . The radial basis function (RBF) was\n",
      "used as the classification method, and the most significant\n",
      "bands for distinguishing between healthy and diseased\n",
      "stages were identified (388nm, 591nm, 646nm, 975nm, and\n",
      "1012nm). The RBF method was able to diagnose Powdery\n",
      "mildew disease even in the asymptomatic phases, achieving a\n",
      "classification accuracy of 82% and 89% for the asymptomatic\n",
      "and early stages, respectively, in both laboratory and field\n",
      "settings. The best classification results were obtained in the\n",
      "very late stages of disease progression, with 96% and 99%\n",
      "accuracy in laboratory and field settings, respectively.\n",
      "Agarwal et al. [51] introduced a CNN model with\n",
      "eight hidden layers that is lightweight and designed for\n",
      "classifying nine types of diseases in tomato crops. The model\n",
      "outperformed traditional ML techniques and pre-trained\n",
      "models with an accuracy of 98.4% on the publicly available\n",
      "PlantVillage dataset, which had 200–1400 images in different\n",
      "classes. The study used various assessment metrics such\n",
      "as accuracy, precision, recall, and F1-score to evaluate the\n",
      "performance of the proposed algorithm. The authors also\n",
      "employed image preprocessing and augmentation techniques\n",
      "to enhance the performance of the CNN. In the preprocessing\n",
      "step, they downsized images to 128 × 128. Overall, the\n",
      "proposed model achieved a high accuracy of 98.4% on the\n",
      "PlantVillage dataset.\n",
      "Anagnostis et al. [52] proposed a CNN model that\n",
      "can categorize images of walnut tree leaves based on\n",
      "whether or not they are infected with anthracnose. For\n",
      "this purpose, a set of grayscale and RGB images were\n",
      "used and Fourier transformation was applied to extract\n",
      "features. A CNN architecture was chosen on the basis of\n",
      "its performance. Feature extraction is significantly impacted\n",
      "by the Fourier transformation in grayscale images, as it\n",
      "emphasizes the abrupt changes and edges of the leaves.\n",
      "Infected leaves are more edgy in comparison to healthy\n",
      "ones. In the preprocessing step, they downsized images to\n",
      "VOLUME 11, 2023 114357\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "256 × 256. The proposed CNN architecture has better or\n",
      "similar performance to widely-used CNN architectures such\n",
      "as ResNet50, VGG16, InceptionV3, and DenseNet121 [53],\n",
      "which are often used as benchmarks in image classification\n",
      "challenges. The accuracy achieved by the proposed CNN\n",
      "architecture was 99%.\n",
      "Chen et al. [54] compared Lasso, RF, gradient boosting,\n",
      "and generalized linear models in predicting the possibility of\n",
      "a high rate of occurrence and severity of grape downy mildew\n",
      "using a dataset of nine years of observations. The algorithms\n",
      "evaluated in this study use the date of initial infection,\n",
      "as well as average monthly temperatures and precipitation\n",
      "as input parameters. Among these algorithms, Lasso, RF,\n",
      "and gradient boosting perform better than generalized linear\n",
      "models. Precipitation is found to have a larger impact on\n",
      "accuracy than temperature among weather inputs, while the\n",
      "date of disease onset has a greater impact on accuracy than\n",
      "weather inputs. The best-performing algorithm is selected\n",
      "based on results for high incidence and severity on leaves\n",
      "and bunches and is used to assess the influence of various\n",
      "climatic scenarios on grape downy mildew risk levels. The\n",
      "study found that reduced rainfall and higher temperatures in\n",
      "April-May decrease the probability of grape downy mildew\n",
      "during bunch closure. The highest area under the ROC curve\n",
      "achieved in this study was 86%.\n",
      "An image processing technique for identifying plant\n",
      "diseases was proposed by Cristin et al. [55] that is efficient.\n",
      "The input image undergoes preprocessing to eliminate noise\n",
      "and artifacts. The preprocessed image is then segmented\n",
      "using a piecewise fuzzy C-means clustering method to\n",
      "obtain the segments. Texture features such as information\n",
      "gain, histogram of oriented gradients, and entropy are\n",
      "extracted from each segment during the feature extraction\n",
      "stage. A Deep Belief Network (DBN) is employed in the\n",
      "classification step, which was trained with the Rider-CSA\n",
      "algorithm, combining the Rider Optimization Algorithm\n",
      "(ROA) with the Cuckoo search method. The Rider-CSA-\n",
      "based DBN method proposed in the study outperformed\n",
      "previously used methods, with a maximum accuracy of 87%,\n",
      "sensitivity of 86%, and specificity of 87% in the experiments.\n",
      "Lee et al. [56] used four CNN models, i.e., VGG16,\n",
      "InceptionV3, GoogLeNet, and GoogLeNetBN [57], for plant\n",
      "disease identification. They utilized three datasets, PlantVil-\n",
      "lage, IPM [7], and Bing [7]. They compared the performance\n",
      "trained from scratch and using transfer learning from existing\n",
      "pre-trained models. In the preprocessing step, they downsized\n",
      "and cropped images to different ratios according to the type of\n",
      "CNN. The VGG16 network performed better than the other\n",
      "CNN architectures, achieving an accuracy of 99% on seen\n",
      "crops and 65% on unseen crops.\n",
      "Giraddi et al. [58] presented a DL system with image\n",
      "processing techniques to detect fungal diseases in maize\n",
      "leaves. They used image classification techniques along with\n",
      "AlexNet, a deep CNN. Their model can detect three different\n",
      "classes of fungal disease, Northern blight, Southern Blight,\n",
      "and Rust. They experimented with two different problems.\n",
      "The first problem is with three classes: healthy, common\n",
      "rust, and Northern blight images, and the second problem is\n",
      "with four classes: healthy, common rust, Northern blight, and\n",
      "multiple diseases in one image. For the first problem, three\n",
      "distinct disease detection models were generated, trained, and\n",
      "evaluated. In the preprocessing step, they downsized images\n",
      "to make them the same size. They used 300 images in the\n",
      "training set, 100 for each class. They also used 150 images\n",
      "in the testing set, 50 for each class. The training set for\n",
      "the second problem consists of 400 images, 100 for each\n",
      "class, while the testing set includes 200 images, 50 for\n",
      "each class. They obtained an accuracy of 98% on the three-\n",
      "class classification problem and an accuracy of 70.5% on the\n",
      "four-class classification problem.\n",
      "A CNN-based approach for plant disease diagnosis and\n",
      "recognition is proposed by Guo et al. [59]. The model\n",
      "is designed to improve accuracy, generality, and training\n",
      "efficiency. The approach first uses the Region Proposal\n",
      "Network (RPN) [60] to detect and locate the leaves in a\n",
      "challenging environment. Then, the Chan-V ese (CV) method\n",
      "[61] is used to segment the images based on the results of the\n",
      "RPN algorithm, which captures the features of the symptoms.\n",
      "Finally, the segmented leaves are fed into the transfer learning\n",
      "model, which is trained using the PBPC [62] of diseased\n",
      "leaves in a simple environment. The approach is tested\n",
      "for black rot, bacterial plaque, and rust disease, achieving\n",
      "an accuracy of 83.75%, which is higher than conventional\n",
      "methods, and can help minimize the impact of diseases on\n",
      "agricultural production.\n",
      "Habib et al. [63] introduced an agromedical expert system\n",
      "for papaya disease identification that relies on machine\n",
      "vision and analyzes images taken using portable devices. The\n",
      "proposed system focuses on two main objectives: disease\n",
      "detection and disease classification. The effectiveness of\n",
      "the system was tested through several experiments. Initially,\n",
      "they defined a set of features that can help differentiate\n",
      "between the different diseases. Then, they used the K-means\n",
      "clustering algorithm to segment the disease-affected area in\n",
      "the collected images and extracted the relevant features to\n",
      "classify the diseases using the support vector machine (SVM)\n",
      "algorithm. In the preprocessing step, they downsized images\n",
      "to 300 × 300. The system achieved an accuracy of 90.15%.\n",
      "Karadağ et al. [64] proposed a method for identifying\n",
      "fusarium disease on peppers utilizing a spectroradiometer\n",
      "from the reflections obtained from pepper leaves in a\n",
      "laboratory. Four sets of pepper leaves were grown in\n",
      "a confined setting to obtain reflections. They proposed a\n",
      "two-step disease detection framework. Subsequently, the\n",
      "extracted feature vector was utilized in the classification\n",
      "process, employing ANNs, Naive Bayes (NB), and KNN. The\n",
      "obtained accuracy for KNN was 100%, 97.5% for ANN, and\n",
      "90% for NB.\n",
      "Karlekar and Seal [65] proposed a computer vision method\n",
      "to identify and categorize leaf diseases present in soybean\n",
      "114358 VOLUME 11, 2023\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "crops. Initially, the proposed approach separates the leaf\n",
      "section by removing the complex background from the entire\n",
      "image. Following this, a deep learning Convolutional Neural\n",
      "Network (CNN) known as SoyNet, trained on a 16-class\n",
      "dataset called PDDB [27], categorizes the segmented leaf\n",
      "images. In the preprocessing step, they downsized images\n",
      "to 100 × 100. The proposed model attains an identification\n",
      "accuracy of 98.14%, with high precision, recall, and F1-score.\n",
      "Karthik et al. [66] proposed two alternative deep neural\n",
      "network (DNN) models to identify the type of infection\n",
      "present in tomato leaves. The first model applies residual\n",
      "learning on top of a feed-forward CNN to learn crucial\n",
      "classification features, while the second model utilizes the\n",
      "strengths of the attention mechanism and residual learning on\n",
      "CNNs. To assess the performance of the proposed models, the\n",
      "PlantVillage dataset was used, which includes three diseases:\n",
      "early blight, late blight, and leaf mold. They reported\n",
      "an accuracy of 95% and 98% for the two architectures,\n",
      "respectively.\n",
      "The detection of mildew disease in vineyards using a\n",
      "CNN on UA V images was proposed by Kerkech et al. [67].\n",
      "Additionally, they introduced a new object recognition\n",
      "technique that aligns visible and infrared images, allowing\n",
      "data from the two sensors to be combined. A fully CNN\n",
      "technique is then used to classify each pixel into different\n",
      "categories, such as ground, shadow, symptom, and healthy.\n",
      "The proposed method was able to detect over 92% of\n",
      "infections at the grapevine level and 87% at the leaf level,\n",
      "indicating that automatic disease detection in vineyards has a\n",
      "promising future.\n",
      "Khalili et al. [68] experimented with six different ML\n",
      "algorithms, namely MLP , Gradient Tree Boosting (GBT),\n",
      "L1 regularization (LR-L1), L2 regularization (LR-L2), SVM,\n",
      "and RF in order to forecast charcoal rot disease in soybean on\n",
      "a custom dataset [69] of 2,000 healthy and diseased plants.\n",
      "The importance of the features considered by various ML\n",
      "methods was also analyzed. They applied a 10-fold cross-\n",
      "validation to the training set and all ML models achieved an\n",
      "accuracy of more than 96.79%.\n",
      "The use of an optimized deep neural network for identi-\n",
      "fying and classifying paddy leaf infections in rice harvests\n",
      "was proposed by Ramesh et al. [70]. The Jaya algorithm\n",
      "was utilized for this study. Images of rice plant leaves from\n",
      "farm fields with normal, sheath rot, blast diseases, brown\n",
      "spots, and bacterial blight were collected. In preprocessing,\n",
      "RGB images were converted to HSV images to eliminate\n",
      "the background, and then binary images based on the hue\n",
      "and saturation sections were retrieved to determine whether\n",
      "or not a disease occurred in a region. Several metrics, such\n",
      "as accuracy, precision, and F1-score, were used to evaluate\n",
      "the results, and the outcomes were compared to those of\n",
      "Denoising Auto-Encoder (DAE), DNN, and ANN classi-\n",
      "fiers. In the preprocessing step, they downsized images to\n",
      "300 × 450. The proposed method achieved high accuracy\n",
      "rates of 98.9% for blast-affected images, 95.7% for bacterial\n",
      "blight, 94% for brown spot, 92% for sheath rot, and 90.57%\n",
      "for normal leaf images, outperforming the other classifiers.\n",
      "V erma et al. [71] trained and compared the CNN models\n",
      "SqueezeNet [72], InceptionV3, and AlexNet, for determining\n",
      "the severity of late blight infection in tomatoes. For this\n",
      "purpose, they utilized the PlantVillage dataset for transfer\n",
      "learning and feature extraction. AlexNet outperformed the\n",
      "other two networks with an accuracy of 93.4%.\n",
      "V elásquez et al.[73] proposed a technique to detect coffee\n",
      "leaf rust by utilizing data from various sources, including\n",
      "sensor data in JSON format, as well as RGB, RGN, and\n",
      "RE images from UA V multispectral cameras. The proposed\n",
      "approach involved the integration of remote sensing and\n",
      "DL algorithms, specifically MLP and CNN. The sub-models\n",
      "were trained separately, and the resulting F1-scores for each\n",
      "sub-model and the composite model were calculated. The\n",
      "composite model achieved an F1-score of 77.5%.\n",
      "Y an et al. [74] presented an enhanced version of the\n",
      "VGG16 model to detect diseases in apple leaves. They\n",
      "utilized a dataset that consisted of 10 plant types with\n",
      "27 different disease categories obtained from the ‘‘2008 ‘AI\n",
      "Challenger’ Global Challenge’’ [75], as well as additional\n",
      "images. The authors employed transfer learning to shorten\n",
      "the learning time. The results demonstrate that the proposed\n",
      "model achieved an overall accuracy of 99.01%.\n",
      "Zhang et al. [76] presented a structured technique for\n",
      "wheat lodging detection in experimental plots that included\n",
      "aerial imagery collected from UA Vs. For classifying wheat\n",
      "lodging, ML algorithms, and CNNs were utilized. For the ML\n",
      "algorithms, 320 extracted features were fed into RF, NN, and\n",
      "SVM. For the CNNs, they used VGGNet and GoogLeNet.\n",
      "GoogLeNet had significantly higher accuracy than VGGNet,\n",
      "about 93%, while SVM had an accuracy of 92%.\n",
      "Abbas et al. [77] proposed a DL approach for detecting\n",
      "diseases on tomato plant leaves. They utilized the Conditional\n",
      "Generative Adversarial Network (CGAN) [78] in order to\n",
      "generate synthetic images. The classification of ten types\n",
      "of diseases on tomato leaves was carried out using the\n",
      "DenseNet121 model trained on both synthetic and real\n",
      "images through transfer learning. In the preprocessing step,\n",
      "they downsized images to 224×224. The study employed the\n",
      "PlantVillage dataset, and the obtained accuracy was 97.11%.\n",
      "An efficient automated diagnosis system for corn plants\n",
      "was proposed by Akanksha et al. [79]. The proposed\n",
      "method consists of four stages: (i) preprocessing, (ii) feature\n",
      "extraction, (iii) classification, and (vi) segmentation. The\n",
      "preprocessing techniques convert images to RGB and remove\n",
      "noise. They utilized an Optimized Probabilistic Neural\n",
      "Network (OPNN) that was improved by using the Artificial\n",
      "Jelly Optimization (AJO) algorithm for classification. The\n",
      "system achieved an accuracy of 95.5%.\n",
      "The automatic detection of bacterial spot diseases in peach\n",
      "plants was proposed by Bedi and Gole [80] using a hybrid\n",
      "model that combines a Convolutional Autoencoder (CAE)\n",
      "network with a CNN. They used the CAE for dimensionality\n",
      "VOLUME 11, 2023 114359\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "reduction to reduce the number of training parameters, and\n",
      "the CNN was used for classification. The model was trained\n",
      "and evaluated on the PlantVillage dataset, achieving an\n",
      "accuracy of 99.35% on the training set and 98.38% on the\n",
      "test set with 9, 914 training parameters.\n",
      "According to the study conducted by Chowdhury et al.\n",
      "[81], tomato leaf images were classified for tomato dis-\n",
      "eases using the EfficientNet CNN architecture [82]. The\n",
      "PlantVillage dataset, containing 18,162 tomato images, was\n",
      "used to fine-tune and train the model to detect healthy and\n",
      "diseased tomato leaf images. In the preprocessing step, they\n",
      "downsized images to 224 × 224. The results indicate that\n",
      "the proposed model outperformed various contemporary DL\n",
      "algorithms. Two segmentation models, the original U-net\n",
      "[83] and modified U-net [84], were also trained, validated,\n",
      "and tested for the segmentation of tomato leaf images. The\n",
      "Modified U-net was found to be superior for segmenting\n",
      "leaf images from the background, and EfficientNetB7 was\n",
      "superior at extracting discriminative features from images.\n",
      "The performance of the models generally improved with\n",
      "the number of parameters used. The trained models have\n",
      "potential applications in automatic detection of plant diseases\n",
      "at an early stage, and the study achieved an accuracy of\n",
      "98.6% for the U-net model, 99.9% for the EfficientNetB7,\n",
      "and 99.8% for the EfficientNetB4.\n",
      "Dwivedi et al. [85] presented a disease detection network\n",
      "for grapes. This network was based on Faster RCNN [60].\n",
      "During the evaluation stage, the proposed disease detection\n",
      "mechanism was experimented on the PlantVillage dataset,\n",
      "and it was found that the network was more efficient in\n",
      "detecting infected/diseased regions than existing methods.\n",
      "The results showed an overall accuracy of 99.93% for\n",
      "identifying esca, black rot, and isariopsis diseases.\n",
      "Mishra et al. [86] developed an automated plant disease\n",
      "detection model using median filter preprocessing techniques\n",
      "and a pixel-level feature extraction technique. In this\n",
      "technique, the statistical features are extracted from an\n",
      "image. Afterward, the researchers utilized the Sine Cosine\n",
      "Algorithm-based Rider Neural Network (SCA-RideNN) as\n",
      "the classifier for detecting plant diseases. They conducted\n",
      "experiments with the PlantVillage dataset and reported an\n",
      "accuracy of around 91.56%.\n",
      "For guava disease detection, Mostafa et al. [87] utilized\n",
      "five neural network models, namely AlexNet, SqueezeNet,\n",
      "GoogLeNet, ResNet50, and ResNet101 [24]. They started\n",
      "by preprocessing the images using the color histogram and\n",
      "unsharp masking method, followed by augmentation using\n",
      "the affine transformation method. In the preprocessing step,\n",
      "they downsized images to be of the same size. The results\n",
      "indicated that ResNet101 achieved the highest classification\n",
      "accuracy of 97.74%.\n",
      "The aim of Patil and Patil [88] was to develop a DL\n",
      "method to detect a diseased cotton plant from its leaf\n",
      "images. They used an IoT-based platform to collect various\n",
      "sensor data to detect climatic changes. Their deep CNN\n",
      "architecture comprised convolutional layers, ReLU layers,\n",
      "fully connected layers, pooling layers, and activation layers.\n",
      "They used a custom dataset and applied image preprocessing,\n",
      "augmentation, and fine-tuning techniques. In the preprocess-\n",
      "ing step, they downsized images to 256 × 256. The proposed\n",
      "method yielded an accuracy of about 98%.\n",
      "The aim of Sambasivam and Opiyo [89] was to detect\n",
      "cassava infections using a CNN model trained on a dataset\n",
      "containing 10,000 annotated images from [90]. The dataset\n",
      "included five fine-grained cassava leaf disease categories;\n",
      "however, there was a significant class imbalance issue with\n",
      "the data. The authors employed several techniques such as\n",
      "class weight, focus loss, and SMOTE to counter the imbal-\n",
      "ance and improve the model’s performance. After tuning\n",
      "several parameters, including the learning rate, number of\n",
      "epochs, batch size, input shape, optimizer, and number of\n",
      "neurons in the hidden layer, they found that a vector input\n",
      "shape of (448, 448, 3) yielded the best accuracy of 93%.\n",
      "Sujatha et al. [91] assessed and compared the performance\n",
      "of various ML and DL algorithms for the detection of diseases\n",
      "in citrus leaf images as described in [92]. The algorithms\n",
      "evaluated included SVM, RF, stochastic gradient descent,\n",
      "InceptionV3, VGG16, and VGG19. The results showed\n",
      "that the VGG16 model achieved the highest classification\n",
      "accuracy of 89.5%, while the RF model had the lowest\n",
      "accuracy of 76.8%.\n",
      "A CNN-based approach proposed by Tiwari et al. [93]\n",
      "was used for the detection and classification of plant diseases\n",
      "from leaf images captured in different resolutions. The study\n",
      "included six crops (apple, potato, tomato, bean, citrus, and\n",
      "rice) and 27 different disease categories under laboratory\n",
      "and on-field conditions. The authors also utilized five-fold\n",
      "cross-validation and tested the model on unseen data to\n",
      "enhance the accuracy of the results. Images were gathered\n",
      "from several databases, such as PlantVillage, iBean leaf\n",
      "image dataset [94], citrus leaf images, and rice leaf images\n",
      "[95]. In the preprocessing step, they downsized images to\n",
      "224 × 224. The cross-validation accuracy was found to be\n",
      "99.58%, and the average test accuracy was 99.12% for images\n",
      "with complex environmental situations.\n",
      "The use of a double GAN for balancing datasets by\n",
      "generating images of diseased plant leaves was proposed by\n",
      "Zhao et al. [96]. Initially, they used healthy leaf images from\n",
      "the PlantVillage dataset as inputs for the Wasserstein GAN\n",
      "to obtain a pre-trained model. Subsequently, diseased leaves\n",
      "were used to generate 64 × 64 pixel images of diseased\n",
      "leaves using the pre-trained model. To expand the unbal-\n",
      "anced dataset, a superresolution GAN was used to obtain\n",
      "256 × 256 pixel images. The authors observed that the\n",
      "generated images from the double GAN were clearer than\n",
      "those from DCGAN. They achieved an accuracy of 99.80%\n",
      "and 99.53% for the double GAN and DCGAN, respectively.\n",
      "Kathole and Munot [97] evaluated and compared the\n",
      "performance of various CNN models, including VGG\n",
      "architecture from scratch, VGG architecture with pre-trained\n",
      "114360 VOLUME 11, 2023\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "weights, GoogLeNet, and AlexNet, for detecting plant\n",
      "diseases in tomato crops. The experiments were conducted\n",
      "on six different disease classes and a healthy class, and the\n",
      "data were obtained from PlantVillage. The GoogLeNet model\n",
      "achieved the highest accuracy of 97.94%.\n",
      "Pan et al. [98] employed multiple CNN models including\n",
      "AlexNet, GoogleNet, VGG16, and VGG19 to detect Northern\n",
      "corn leaf blight in maize crops. The researchers utilized a\n",
      "dataset consisting of 985 images of healthy and infected\n",
      "maize leaves and applied various data augmentation tech-\n",
      "niques to augment their dataset. Their results showed that\n",
      "the GoogleNet model achieved the best performance, with an\n",
      "accuracy of 99.94% in detecting Northern corn leaf blight.\n",
      "The ResTS (Residual Teacher/Student) architecture, devel-\n",
      "oped by Shah et al. [99], is a CNN structure that incorporates\n",
      "two classifiers (ResTeacher and ResStudent) and a decoder\n",
      "to aid in the diagnosis of plant diseases through visualization\n",
      "and classification. They utilized residual connections and\n",
      "performed batch normalization on all subsystems. The\n",
      "PlantVillage dataset was used to train their model, and they\n",
      "achieved an F1-score of 99.1%.\n",
      "Two deep feature extraction-based classification models\n",
      "were proposed by Turkoglu et al.[100] using transfer learning\n",
      "on six pre-trained DL networks (AlexNet, DenseNet201,\n",
      "GoogleNet, ResNet18 [24], ResNet50, and ResNet101) for\n",
      "classifying plant disease images. They also proposed a hybrid\n",
      "CNN-SVM model that combined the early fusion of features\n",
      "extracted from deep networks. The images used for training\n",
      "and testing were from the Turkey-PlantDataset [101], which\n",
      "consists of unconstrained images of 15 types of diseases and\n",
      "pests observed in Turkey. The highest accuracy achieved by\n",
      "all the models was 97.56\n",
      "The automatic plant disease detection technique proposed\n",
      "by V allabhajosyula et al. [102] utilizes Deep Ensemble\n",
      "Neural Networks (DENN) and employs data augmentation\n",
      "techniques, including image enhancement, rotation, scaling,\n",
      "and translation. The researchers used the PlantVillage\n",
      "dataset and concluded that DENN’s performance sur-\n",
      "passes that of pre-trained state-of-the-art models, such as\n",
      "ResNet50, ResNet101, InceptionV3, DenseNet 121 and 201,\n",
      "MobileNetV3 [103], and NasNet [104]. The DENN model\n",
      "achieved an accuracy of 99.99\n",
      "B. OBJECT DETECTION\n",
      "Fuentes et al. [105] combined three object detection\n",
      "algorithms (Faster RCNN, R-FCN [106], and SSD [107])\n",
      "with five feature extractors (VGG16, ResNet50, ResNet101,\n",
      "ResNet152 [24], ResNeXt-50) for recognizing tomato plant\n",
      "diseases and pests. They applied the CNN algorithms on a\n",
      "custom dataset of about 5, 000 images using various methods\n",
      "to augment the dataset size. The results show that R-FCN with\n",
      "ResNet50 achieved the best mAP of 86%.\n",
      "Jiang et al. [108] used a DL approach based on the\n",
      "GoogLeNet Inception structure and the rainbow concatena-\n",
      "tion for apple leaf disease detection. They generated a custom\n",
      "dataset of 2, 029 images of diseased apple leaves and trained\n",
      "their algorithm to detect five common apple leaf diseases:\n",
      "Alternaria leaf spot, brown spot, mosaic, grey spot, and rust.\n",
      "V arious augmentation techniques have been applied in order\n",
      "to generate a more representative dataset. They achieved a\n",
      "detection performance of 78.80% mAP .\n",
      "Kim et al. [109] proposed a system for field monitoring\n",
      "with a PTZ camera, a motor system, a wireless transceiver,\n",
      "and an image-logging module. This system periodically\n",
      "captures images from the field and sends them to a CNN\n",
      "model designed to detect onion-downy mildew diseases. The\n",
      "model was trained on a dataset of six classes of images\n",
      "captured by the field monitoring system. In the preprocessing\n",
      "step, they extracted 224 × 224 samples from images. The\n",
      "proposed model achieved an accuracy ranging from 74.1%\n",
      "to 87.2%.\n",
      "Li et al. [110] presented a technique for disease detection\n",
      "on rice crops using a CNN. They gathered images and\n",
      "videos from mobile phones. They converted videos to a\n",
      "still frame, then transmitted it to a still-image detector for\n",
      "recognition, and then combined the frames into a video. They\n",
      "utilized Faster RCNN as the framework for the still-image\n",
      "detector. The results revealed that the proposed method with\n",
      "the custom backbone was better than ResNet50, ResNet101,\n",
      "YOLOv3 [111], and VGG16 for recognizing diseases and\n",
      "pests for rice crops.\n",
      "Saleem et al. [112] combined three object detection\n",
      "algorithms (Faster RCNN, R-FCN, and SSD) with four\n",
      "feature extractors (ResNet50, ResNet101, InceptionV2, and\n",
      "Inception-ResNetV2 [113]) and three optimizers (stochastic\n",
      "gradient descent with momentum, Adam, and RMSProp) for\n",
      "plant disease identification. They used the training weights of\n",
      "the Common Objects in Context (COCO) dataset [114]. They\n",
      "compared the algorithms on the PlantVillage dataset. The\n",
      "SSD model with the feature extractor InceptionV2 trained\n",
      "with the Adam optimizer was the best performer with a mAP\n",
      "of 73.07%.\n",
      "The detection of Northern maize leaf blight disease on\n",
      "maize crops under complex field environments was proposed\n",
      "by Sun et al. [115] using multi-scale feature fusion and the\n",
      "SSD algorithm. For this purpose, a custom dataset consisting\n",
      "of 18,000 images captured by a camera mounted either on\n",
      "a five-meter boom or on a UA V was used. The proposed\n",
      "technique consists of three steps: preprocessing the dataset,\n",
      "fine-tuning the model, and detecting the disease. The NLB\n",
      "dataset was selected because it is calibrated by human\n",
      "plant pathologists and has a high level of accuracy. Data\n",
      "preparation was done mainly to reduce the effect of high-\n",
      "intensity light on image identification and enhance detection\n",
      "accuracy. The proposed model achieved a total accuracy\n",
      "of 91.83%.\n",
      "Xie et al. [116] presented a DL-based detector for\n",
      "identifying leaf diseases in grape crops. In order to improve\n",
      "the generalization of the model, they developed the grape\n",
      "leaf disease dataset, which included 4,449 original images\n",
      "captured both in the lab and in actual vineyards, and\n",
      "VOLUME 11, 2023 114361\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "expanded the dataset with an additional 62,286 diseased\n",
      "leaf images. The authors trained the model using data aug-\n",
      "mentation techniques and utilized the InceptionV1 module\n",
      "[117], Inception-ResNetV2 module, and SE-blocks [118] to\n",
      "enhance the detection performance of multi-scale and smaller\n",
      "diseased spots. The model achieved an overall accuracy\n",
      "of 81.1%.\n",
      "The detection of tomato diseases and healthy leaves\n",
      "was the objective of the study by Zhang et al. [119].\n",
      "For this purpose, they proposed an improved version of\n",
      "Faster RCNN, a popular object detection algorithm, which\n",
      "utilized a depth residual network instead of VGG16 for\n",
      "image feature extraction. They also employed the K-means\n",
      "clustering algorithm to cluster the bounding boxes obtained\n",
      "from the network. Three feature extraction networks were\n",
      "tested in the experiments, namely VGG16, Mobile, and\n",
      "Res101. The proposed model achieved an mAP of 98.54%\n",
      "when trained with the Res101 network and k-means\n",
      "clustering.\n",
      "Roy and Bhaduri[120] proposed a multi-class plant disease\n",
      "DL model using techniques from the field of object detection.\n",
      "The model was designed to achieve a balance between\n",
      "detection speed and accuracy, and it has been used to identify\n",
      "many classes of apple plant diseases in a real-world setting.\n",
      "The detection rate of 56.9 frames per second was achieved,\n",
      "while improving the mAP and F1-score up to 91.2% and\n",
      "95.9%, respectively.\n",
      "Selvaraj et al. [121] proposed a pixel-based classification\n",
      "method coupled with ML models for identifying banana\n",
      "crops in complex African environments using multi-level\n",
      "satellite images and UA Vs. They used RF for pixel-based\n",
      "classification, combining vegetation indices and PCA, and\n",
      "developed a mixed-model approach using RetinaNet and a\n",
      "customized classifier for simultaneous banana localization\n",
      "and disease classification with UA V -RGB aerial images.\n",
      "The proposed method achieved high accuracy of 99.4%,\n",
      "92.8%, 93.3%, and 90.8% for banana bunchy top disease,\n",
      "Xanthomonas wilt of banana, healthy banana cluster, and\n",
      "individual banana plants, respectively, according to their\n",
      "mixed-model results.\n",
      "The PlantVillage dataset was used by Wang et al. [122]\n",
      "to propose three novel plant disease detection techniques,\n",
      "namely Squeeze-and-Excitation SSD (SE_SSD), Deep Block\n",
      "SSD (DB_SSD), and Deep Block Attention (DBA_SSD).\n",
      "The authors compared the performance of their proposed\n",
      "algorithms with YOLOv3, YOLOv4 [123], YOLOv4 tiny,\n",
      "and Faster RCNN. The mAP obtained for the proposed\n",
      "techniques were 90.77% for SE_SSD, 89.93% for DB_SSD,\n",
      "and 92.20% for DBA_SSD.\n",
      "An enhanced model for plant disease recognition\n",
      "based on the YOLOv5 network model was proposed by\n",
      "Chen et al. [124]. They made changes to the neck module,\n",
      "the SE module, and the loss function. They collected\n",
      "2, 375 images of rubber tree diseases. Their dataset consists\n",
      "of 1, 203 powdery mildew images and 1, 172 anthracnose\n",
      "images. In the preprocessing step, they downsized images\n",
      "to 640 × 640. The improved YOLOv5 network model\n",
      "achieved an accuracy of 70%.\n",
      "The YOLOv4 algorithm was enhanced for detecting\n",
      "diseases in tomato plants by Roy et al. [125]. The researchers\n",
      "utilized DenseNet as the backbone of the modified algorithm\n",
      "and added two new residual blocks to the backbone. They\n",
      "evaluated the proposed model using 1,200 images from\n",
      "the PlantVillage dataset and compared its performance with\n",
      "other object detection methods. The results showed that the\n",
      "proposed model outperformed other state-of-the-art methods\n",
      "with a mAP of 96.29%.\n",
      "The proposed method by Wang et al. [126] utilizes a\n",
      "lightweight version of the YOLOv5 model and includes an\n",
      "improved attention sub-module to enhance both accuracy\n",
      "and efficiency. Additionally, they employed the Ghostnet\n",
      "structure to reduce computations and replaced the origi-\n",
      "nal FPN/PANet structure with the BiFPN structure. The\n",
      "algorithm was evaluated on a custom dataset for plant disease\n",
      "detection, and its performance was compared to other models.\n",
      "The results indicate that the proposed model achieved an\n",
      "accuracy of 92.57%.\n",
      "III. CLASSIFICATION SCHEME\n",
      "In this section, we propose a classification scheme and\n",
      "categorize all relevant works in the associated classes.\n",
      "In addition, we present statistics of the most common\n",
      "crops/data types/methods/algorithms/datasets/metrics used\n",
      "by the reviewed papers. The classes that we use in our\n",
      "classification scheme are the following:\n",
      "• Crop: the type of the crop that each study uses as a case\n",
      "study\n",
      "• Input data: the type of the input data used in the ML\n",
      "algorithms\n",
      "• Dataset: the name of the dataset that is used if this\n",
      "information is available in the paper\n",
      "• Models/Algorithms: the ML algorithm that was used\n",
      "• Method: whether classification (C) or object detection is\n",
      "targeted (C)\n",
      "• Metrics: the evaluation metrics employed to measure the\n",
      "performance of the ML algorithms\n",
      "• Results: a brief description of the results obtained by the\n",
      "ML algorithms\n",
      "Table 7 in Appendix presents all the related work using the\n",
      "above classification scheme. Figures2–7 show the percentage\n",
      "of papers in the different classes. Most papers have focused\n",
      "on disease detection in tomato plants. There are also various\n",
      "works that consider multiple crops. Corn, apple, grape, and\n",
      "rice are crops that have also attracted much attention in\n",
      "the literature. Regarding the input data type, it is obvious\n",
      "that most researchers use RGB images as input to their\n",
      "models/algorithms since most datasets include only such\n",
      "images. Regarding the datasets used in the studies, the\n",
      "majority of studies use a custom dataset, showing that there is\n",
      "a need to have publicly available datasets that can be utilized\n",
      "for benchmarking purposes. The PlantVillage dataset is also\n",
      "widely used.\n",
      "114362 VOLUME 11, 2023\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "FIGURE 2. Statistics of crops.\n",
      "FIGURE 3. Statistics of input data types.\n",
      "FIGURE 4. Statistics of datasets.\n",
      "CNN is the most common neural network architecture\n",
      "used throughout the studies. There are also some works\n",
      "dealing with classifying diseases that use SVM, ANN, RF,\n",
      "KNN, and MLP algorithms. Regarding the method that each\n",
      "study targets, classification is the clear winner compared to\n",
      "object detection. Different metrics have been employed in the\n",
      "literature, with accuracy, F1-score, precision, and recall being\n",
      "among the most commonly used ones.\n",
      "FIGURE 5. Statistics of models/algorithms.\n",
      "FIGURE 6. Statistics of methods.\n",
      "FIGURE 7. Statistics of metrics.\n",
      "IV. DATASETS\n",
      "This section provides an overview of publicly available\n",
      "datasets. These datasets serve different purposes; some of\n",
      "VOLUME 11, 2023 114363\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "them are used for classification to determine if a plant\n",
      "image is healthy or infected with a disease (discussed in\n",
      "Subsection IV -A), while others are used for object detection\n",
      "to identify diseases on plants (discussed in Subsection IV -B).\n",
      "A. CLASSIFICATION\n",
      "The PlantVillage dataset [10] comprises 54,303 leaf images,\n",
      "both healthy and diseased, categorized into 38 classes based\n",
      "on species and diseases. The dataset includes images of\n",
      "14 crop species, such as apple, blueberry, cherry, corn,\n",
      "grape, orange, peach, bell pepper, potato, raspberry, soybean,\n",
      "squash, strawberry, and tomato. It covers 17 fungal-related\n",
      "diseases, four bacterial diseases, two mold (oomycete)\n",
      "diseases, two viral diseases, and one mite-related disease.\n",
      "Additionally, it provides images of disease-free healthy\n",
      "leaves from 12 crop species.\n",
      "The iBean leaf image dataset [94] comprises images of\n",
      "bean leaves captured from field conditions. The National\n",
      "Crops Resources Research Institute (NaCRRI), the national\n",
      "organization in charge of research in agriculture in Uganda,\n",
      "and the Makerere AI lab collaborated to capture these images\n",
      "in several areas of Uganda. Images were captured from\n",
      "the field or garden using a simple smartphone, which were\n",
      "then analyzed by NaCRRI experts who determined which\n",
      "illness was present in each image. The dataset consists of\n",
      "1, 296 images and three classes. 428 images are for the\n",
      "healthy class, 432 images are for the angular leaf spot, and\n",
      "the remaining 436 for the bean rust.\n",
      "The PlantLeaves dataset [127] is comprised of\n",
      "4, 503 images of plant leaves, both healthy and diseased,\n",
      "categorized into 22 categories based on the species and\n",
      "the state of health. The dataset includes 2, 278 healthy leaf\n",
      "images and 2, 225 diseased ones. The images were captured\n",
      "using a basic digital camera.\n",
      "The PlantaeK dataset [128] is a leaf database of indige-\n",
      "nous plants found in Jammu and Kashmir. It comprises\n",
      "2, 153 images of healthy and diseased plant leaves, cate-\n",
      "gorized into 16 groups by species and health status. The\n",
      "images feature various crop species, including apple, apricot,\n",
      "cherry, cranberry, grapes, peach, pear, and walnut. The dataset\n",
      "comprises 1, 223 healthy leaf images and 934 diseased leaf\n",
      "images.\n",
      "The Plant Pathology 2020 challenge dataset [129] is a clas-\n",
      "sification dataset for the foliar disease of apples. The creators\n",
      "of the dataset manually captured 3, 651 real-world symptoms\n",
      "of several apple foliar diseases with varied lighting, angles,\n",
      "surfaces, and noise. The dataset includes 865 healthy leaves,\n",
      "187 cases of complex diseases, 1, 200 cases of apple scab,\n",
      "and 1, 399 cases of cedar apple rot.\n",
      "The citrus leaf images dataset [92] contains images of\n",
      "healthy and infected citrus plants with diseases such as black\n",
      "spot, canker, scab, greening, and melanosis. The dataset\n",
      "includes 609 images from citrus leaves, of which 58 are\n",
      "healthy images, and 150 images from citrus fruits, of which\n",
      "22 are healthy images.\n",
      "The Kaggle dataset [90] contains 9, 436 annotated images\n",
      "and 12, 595 unlabeled images of cassava leaves. The dataset\n",
      "contains five classes, one is the class for healthy plants\n",
      "and the other four are for diseases (cmd, cgm, cbsd, and\n",
      "cbb). NaCRRI in collaboration with the AI lab at Makerere\n",
      "University captured and annotated these images.\n",
      "The dataset of Rice leaf images [95] includes 120 images\n",
      "collected from a village in India. The dataset contains\n",
      "40 images of each disease, for a total of 120 images. The NLB\n",
      "dataset [130] is comprised of 234 images of leaf spot disease\n",
      "in maize crops.\n",
      "B. OBJECT DETECTION\n",
      "The PlantDoc dataset [131] includes 2, 345 images. These\n",
      "images contain 13 plant species and 18 classes of diseases.\n",
      "This dataset is publicly available for download and it can also\n",
      "be used as an open dataset for benchmarks. The classes of\n",
      "the PlantDoc dataset are the following: Cherry leaf, Peach\n",
      "leaf, Cherry leaf, Peach leaf, Corn leaf blight, Apple rust leaf,\n",
      "Potato leaf late blight, Strawberry leaf, Corn rust leaf, Tomato\n",
      "leaf late blight, Tomato mold leaf, Potato leaf early blight,\n",
      "Apple leaf, Tomato leaf yellow virus, Blueberry leaf, Tomato\n",
      "leaf mosaic virus, Raspberry leaf, Tomato leaf bacterial spot,\n",
      "Squash Powdery mildew leaf, Grape leaf, Corn Gray leaf\n",
      "spot, Tomato Early blight leaf, Apple Scab Leaf, Tomato\n",
      "Septoria leaf spot, Tomato leaf, Soyabean leaf, Bell pepper\n",
      "leaf spot, Bell pepper leaf, grape leaf black rot, Potato leaf,\n",
      "and Tomato two-spotted spider mites leaf. PlantDoc contains\n",
      "8, 851 annotations with an average of 3.4 annotations per\n",
      "image. The average image size is 0.53 mp and the distribution\n",
      "of the image sizes starts from 0.01 mp to 24.00 mp. The\n",
      "median image ratio is 800×675. The balance of the classes of\n",
      "PlantDoc is presented in Figure 8. The classes Blueberry leaf,\n",
      "Tomato leaf yellow virus, and Peach leaf are overrepresented\n",
      "with more than 600 images for each class and the classes\n",
      "Tomato leaf late blight, Tomato Early blight leaf, Apple rust\n",
      "leaf, Apple Scab Leaf, grape leaf black rot, Corn rust leaf,\n",
      "Corn Gray leaf spot, Soybean leaf, Potato leaf, and Tomato\n",
      "two-spotted spider mites leaf are under-represented with less\n",
      "than 220 images for each class. A sample of four images with\n",
      "their annotations are shown in Figure 9.\n",
      "The CropDeep dataset[132] is composed of 31,147 images\n",
      "containing over 49, 000 annotated instances from 31 different\n",
      "classes. The images were captured in greenhouses under\n",
      "various conditions using different cameras. Additionally, the\n",
      "IP102 dataset [133] is a comprehensive benchmark dataset for\n",
      "recognizing insect pests. It comprises over 10, 000 images\n",
      "divided into 102 categories, with insect pests that mainly\n",
      "target one agricultural product grouped together in the same\n",
      "top-level category. The IP102 dataset has a hierarchical\n",
      "taxonomy.\n",
      "Table 1 presents the statistics for each dataset including:\n",
      "(i) the name of the dataset, (ii) the type of the crop, (iii) the\n",
      "number of images, (iv) the number of classes, and (v) whether\n",
      "classification (C) or object detection is targeted (C).\n",
      "114364 VOLUME 11, 2023\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "FIGURE 8. Class balance of the PlantDoc dataset.\n",
      "FIGURE 9. Visualization of the PlantDoc dataset with image annotations.\n",
      "TABLE 1. Datasets statistics.\n",
      "V. COMPUTATIONAL STUDY\n",
      "In this computational study, we train five state-of-the-art\n",
      "object detection algorithms and eighteen state-of-the-art\n",
      "classification algorithms on the PlantDoc dataset. Before\n",
      "starting the training process, we applied a prepossessing step.\n",
      "At the PlantDoc dataset, there was a total of 28 annotations\n",
      "that needed to be fixed. The bounding boxes were out of\n",
      "frame in some images, thus they were trimmed to match\n",
      "the image’s edges. Some images had bounding zero pixels\n",
      "and were afterwards dropped. The training set contained\n",
      "25 of these, whereas the test set contained three. After\n",
      "that, we applied image resizing, auto-orientation, and image\n",
      "denoising.\n",
      "All computational experiments were performed on an Intel\n",
      "Xeon Silver 4214 CPU processor with 128 GB of main\n",
      "memory and 48 cores, a clock of 2.2 GHz, running under\n",
      "Ubuntu 20.10 64-bit, and on a NVIDIA Tesla V100 GPU\n",
      "with 32 GB GDDR5 and a core clock of 1683 MHz.\n",
      "For the training process, we utilized Fastai’s learn.lr_find()\n",
      "[134] method to find the optimal learning rate for each\n",
      "algorithm. The idea of this method comes from the Train\n",
      "Learner over a few iterations [135]. This method starts with\n",
      "a very low start_lr and changes it at each mini-batch until it\n",
      "reaches a very high end_lr. We record the loss at each iteration\n",
      "and check how it relates to the learning rate to determine the\n",
      "optimal value before the loss diverges.\n",
      "For the object detection problem, we trained and eval-\n",
      "uated the performance and the accuracy of the following\n",
      "algorithms:\n",
      "1) EfficientDet [136] includes a collection of eight highly\n",
      "efficient and accurate algorithms developed by Google.\n",
      "EfficientDet detectors are single-shot detectors like\n",
      "SSD and RetinaNet. The EfficientNet, trained in\n",
      "the ImageNet [137] dataset, serves as the network’s\n",
      "backbone. The model utilizes a BiFPN as the feature\n",
      "network, while a separate class and box network is\n",
      "responsible for generating the class and bounding box\n",
      "predictors. On the COCO dataset, the overall mAP of\n",
      "EfficientDet-D7 is 52.2%.\n",
      "2) Faster RCNN [60] belongs to the two-stage detector\n",
      "family because its object detection work occurs in two\n",
      "major stages. Faster RCNN is slower than single-stage\n",
      "detectors, but it is more accurate. On the COCO dataset,\n",
      "the overall mAP@.5 is 42.7%.\n",
      "3) RetinaNet [138], a single-stage object detection model,\n",
      "is considered one of the top performers in detecting\n",
      "both dense and small objects. It has two main improve-\n",
      "ments over other single-stage models: FPN [139] and\n",
      "Focal Loss [138]. On the COCO dataset, RetinaNet’s\n",
      "mAP@.5 is 59.1%.\n",
      "4) SSD [107] is based on the VGG16 architecture but lacks\n",
      "fully connected layers. For object detection, it employs\n",
      "a single deep neural network. The VGG architecture\n",
      "was employed as the foundation, and several layers\n",
      "were utilized for prediction at various scales. On the\n",
      "COCO dataset, the mAP@.5 is 46.5%.\n",
      "5) YOLOv5 [140] is a notable upgrade from YOLO[141],\n",
      "an object detection algorithm that partitions images\n",
      "into a grid system, with each grid cell responsible for\n",
      "detecting objects within it. YOLOv5 is a significant\n",
      "improvement over its predecessor. YOLOv5 uses the\n",
      "PANet structure that includes various layers. On the\n",
      "COCO dataset, the average accuracy is 43.5% AP .\n",
      "Table 2 provides a summary of the hyperparameters used\n",
      "for the trained object detection algorithms in our study.\n",
      "VOLUME 11, 2023 114365\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "The hyperparameters include the learning rate (LR), batch\n",
      "size (BS), optimizer, and backbone architecture. The choice\n",
      "of hyperparameters for our trained object detection algo-\n",
      "rithms played a pivotal role in configuring and effectively\n",
      "training our models for leaf disease detection. Rather than\n",
      "relying on default values, we meticulously considered each\n",
      "hyperparameter based on its impact on model performance.\n",
      "For the EfficientDet algorithm, a learning rate (LR) of\n",
      "5e-2 was chosen with a batch size (BS) of 8. The Adam\n",
      "optimizer was utilized, and the EfficientNet served as the\n",
      "backbone architecture. This configuration likely leverages\n",
      "the efficient architecture and large learning rate to achieve\n",
      "rapid convergence. In the case of Faster RCNN, a lower\n",
      "learning rate of 2e-4 was employed along with a batch size\n",
      "of 16. Stochastic Gradient Descent (SGD) [142] was selected\n",
      "as the optimizer, and ResNet-50 served as the backbone.\n",
      "The lower learning rate and larger batch size indicate a\n",
      "more stable and gradual training process. For RetinaNet,\n",
      "a smaller learning rate of 7e-5 was used with a batch size\n",
      "of 8. Similar to EfficientDet, the Adam [143] optimizer was\n",
      "chosen, and ResNet-50 [24] was employed as the backbone\n",
      "architecture. This configuration may favor a balance between\n",
      "rapid convergence and fine-tuning. The SSD algorithm used\n",
      "a learning rate of 7e-5 with a larger batch size of 32. The\n",
      "Adam optimizer was also selected, but the VGG16 [21]\n",
      "architecture was chosen as the backbone. This combination\n",
      "of parameters aims at accommodating a larger batch size\n",
      "while leveraging the SSD architecture. Lastly, YOLOv5 used\n",
      "a relatively high learning rate of 3e-3 with a batch size of 32.\n",
      "Similar to the other models, Adam was used as the optimizer,\n",
      "and the new CSP-Darknet53 [144] architecture served as the\n",
      "backbone, a modification of the Darknet architecture used\n",
      "in previous versions [123]. This configuration may achieve\n",
      "rapid convergence and adapt to the YOLO architecture.\n",
      "TABLE 2. Hyperparameters of the algorithms.\n",
      "Table 3 presents the mean average precision (mAP) results\n",
      "of the leaf disease detection models after training.\n",
      "To evaluate the overall accuracy of the object detector\n",
      "models, we utilized several COCO challenge metrics, namely\n",
      "(i) AP , (ii) APIoU =.50, (iii) APIoU =.75, (iv) AP small, (v) AP\n",
      "medium, and (vi) AP large. The assessment results for the\n",
      "COCO metrics are presented in Table 4.\n",
      "Based on the results, it is evident that YOLOv5 exhibits\n",
      "superior accuracy compared to the other algorithms, as it\n",
      "outperforms them in terms of detecting objects of all sizes in\n",
      "the images. This superiority is reflected not only in the strict\n",
      "AP metric of the COCO challenge but also in other evaluation\n",
      "TABLE 3. mAP of the trained leaf disease detection models.\n",
      "TABLE 4. Average precision on COCO metrics.\n",
      "FIGURE 10. YOLOv5 ground truth vs prediction visualization.\n",
      "metrics such as small, medium, and large object detection.\n",
      "EfficientDet, Faster RCNN, RetinaNet, and SSD have almost\n",
      "the same accuracy for this dataset. From the visualization\n",
      "of the results, it is shown that all algorithms were able to\n",
      "identify and localize most of the leaf species and diseases.\n",
      "However, there are images that show that the detection results\n",
      "are suboptimal, as shown in Figure 10.\n",
      "114366 VOLUME 11, 2023\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "TABLE 5. Hyperparameters for classification models.\n",
      "We trained and evaluated eighteen CNN model archi-\n",
      "tectures that are currently considered state-of-the-art for\n",
      "the image classification problem. AlexNet, DenseNet (121,\n",
      "161, 169, 201), MobileNetV2, ResNet (50, 101, 151),\n",
      "ResNeXt (50, 101) [145], ShuffleNet [146], VGG, and\n",
      "WideResNet (50, 101) [147]. Eighteen state-of-the-art CNN\n",
      "model architectures pre-trained on the ImageNet dataset were\n",
      "trained and evaluated for the image classification problem.\n",
      "The models were trained using the same hyperparameters for\n",
      "200 epochs and were assessed based on their accuracy and\n",
      "training time. The Stochastic Gradient Descent optimizer and\n",
      "the softmax cross entropy loss function were utilized by all\n",
      "models.\n",
      "The hyperparameters presented in Table 5 for the various\n",
      "classification models were selected after careful experimenta-\n",
      "tion and tuning to achieve optimal model performance. These\n",
      "hyperparameters, including the learning rate (LR), batch size\n",
      "(BS), and optimizer, play a crucial role in determining the\n",
      "effectiveness of the training process and ultimately impact the\n",
      "model’s accuracy and training times. The choice of a learning\n",
      "rate of 0.001 is a common starting point for many deep-\n",
      "learning models. It is often used as a default value and then\n",
      "fine-tuned during experimentation. The batch size of 32 or\n",
      "64 is also a result of experimentation. Smaller batch sizes may\n",
      "provide more stable convergence, but larger batch sizes can\n",
      "lead to faster training times. As for the optimizer, models like\n",
      "AlexNet and MobileNetV2 benefit from the Adam optimizer,\n",
      "which adapts learning rates individually for each parameter.\n",
      "On the other hand, models like DenseNet and ResNet variants\n",
      "perform well with the Stochastic Gradient Descent (SGD)\n",
      "optimizer. The choice of optimizer depends on the model\n",
      "architecture and its convergence characteristics.\n",
      "TABLE 6. Comparison of classification models in terms of accuracy and\n",
      "training time.\n",
      "Table 6 presents the results. The primary evaluation metric\n",
      "in this computational study is accuracy, which is the ratio\n",
      "of the number of correct predictions to the total number of\n",
      "predictions.\n",
      "According to the study’s results, the DenseNet121 model\n",
      "outperformed all other models by achieving the high-\n",
      "est accuracy of 61.01%. DenseNet161, ResNet152, and\n",
      "WideResNet50_2 also performed well, achieving an accuracy\n",
      "of 60.16%. On the other hand, ShuffleNet_v2_x2_0 had the\n",
      "worst accuracy of 20.33% on the PlantDoc dataset.\n",
      "The validation accuracy curve of all the models from epoch\n",
      "0 to epoch 200 is depicted in Figure 11, whereas Figure 12\n",
      "displays the training accuracy curve of all the models. All of\n",
      "the networks achieved a training accuracy of more than\n",
      "97%, except for AlexNet which was not able to reach more\n",
      "than 85% of training accuracy. Finally, we compared all of\n",
      "the training times in Figure 13. MobileNetv2, ResNet50,\n",
      "and DenseNet121 had the shortest training times of 67m,\n",
      "73m, and 77m, respectively, for an accuracy of more than\n",
      "55%. The ideal balance between accuracy and training time\n",
      "was achieved from DenseNet121 by achieving the highest\n",
      "accuracy and one of the fastest training times. The reason\n",
      "for DenseNet’s superior performance is that it requires fewer\n",
      "parameters than traditional CNNs, eliminating the need to\n",
      "learn redundant feature maps. Moreover, some variations\n",
      "of ResNet models have revealed that several layers do\n",
      "not contribute much and can be discarded. Finally, in the\n",
      "surveyed papers presented in the literature review, there\n",
      "are no algorithms that have been tested in the PlantDoc\n",
      "dataset. However, many studies use the PlantVillage dataset.\n",
      "The algorithms used in the PlantVillage dataset for the\n",
      "classification category, specifically for RGB images, achieve\n",
      "VOLUME 11, 2023 114367\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "FIGURE 11. Comparison of the validation accuracy of the classification\n",
      "models.\n",
      "FIGURE 12. Comparison of the training accuracy of the classification\n",
      "models.\n",
      "FIGURE 13. Total training time of each classification model.\n",
      "a performance accuracy of up to 99% [7], [13], [15].\n",
      "In contrast, the algorithms utilized in the PlantDoc dataset\n",
      "reach a maximum accuracy of 60% for the same category.\n",
      "Moving on to the object detection category, the algorithms\n",
      "employed in the PlantVillage dataset demonstrate an accuracy\n",
      "of up to 96% [112], [122], [125]. However, the algorithms\n",
      "utilized in the PlantDoc dataset achieve a lower accuracy of\n",
      "up to 56% for object detection. The size and diversity of\n",
      "the dataset can significantly impact algorithm performance.\n",
      "The PlantVillage dataset is larger and contains a more diverse\n",
      "set of images, thus it provides the algorithms with a broader\n",
      "range of examples to learn from. A larger dataset can help the\n",
      "algorithms generalize better to new, unseen data. Both in the\n",
      "classification method and in the object detection method we\n",
      "use the algorithms that are state-of-the-art and which are also\n",
      "used in other studies for different datasets.\n",
      "VI. DISCUSSION\n",
      "In the field of computer vision, the decision-making process\n",
      "for selecting appropriate methodologies often takes into\n",
      "account the trade-offs between classification and object\n",
      "detection methods. The classification method serves as a\n",
      "foundational concept in machine learning, providing sim-\n",
      "plicity and interpretability. It allows for the straightforward\n",
      "categorization of inputs into predefined classes. Additionally,\n",
      "its computational efficiency makes it suitable for applications\n",
      "with limited resources. However, the classification method\n",
      "does have its limitations. It lacks fine-grained information,\n",
      "such as spatial details or the ability to handle multiple objects\n",
      "simultaneously. This makes it less suitable for tasks that\n",
      "require precise localization or the identification of multiple\n",
      "objects at once. On the other hand, object detection methods\n",
      "not only provide class labels but also offer precise spatial\n",
      "information, which addresses the challenge of localization.\n",
      "These methods excel in scenarios where multiple objects need\n",
      "to be handled and are versatile for tasks like instance seg-\n",
      "mentation and keypoint detection. Despite their advantages,\n",
      "object detection methods are more complex to implement.\n",
      "They require bounding box regression and labor-intensive\n",
      "data annotation. Moreover, they often demand significant\n",
      "computational resources, which restricts their deployment in\n",
      "real-time and resource-constrained environments. The choice\n",
      "between classification and object detection depends on the\n",
      "available data and the specific requirements of the task.\n",
      "High-performing disease detection algorithms are pivotal\n",
      "in precision agriculture, aiding early disease identification,\n",
      "reducing crop losses, and efficient resource allocation. They\n",
      "precisely detect diseases like fungal infections and viral\n",
      "outbreaks, minimizing the need for extensive treatments.\n",
      "Their implementation may require specific hardware, e.g.,\n",
      "GPUs, and robust computational resources, with infrastruc-\n",
      "ture and cost considerations being crucial. Moreover, quality\n",
      "labeled datasets, capturing diverse disease manifestations\n",
      "and environmental conditions, are vital for accurate model\n",
      "training. Finally, the integration with existing precision\n",
      "agriculture systems, through standardized interfaces and\n",
      "APIs, and user-friendliness, with intuitive interfaces and\n",
      "interpretable outputs, empowers farmers and practitioners,\n",
      "fostering trust. In practical terms, several projects exemplify\n",
      "the application of these advanced algorithms in precision\n",
      "agriculture:\n",
      "• PlantVillage [10] utilizes machine learning and com-\n",
      "puter vision for disease detection in crops. Its user-\n",
      "friendly mobile application enables farmers to take\n",
      "pictures of their crops and receive automated disease\n",
      "diagnoses. This project showcases the practical appli-\n",
      "cation of classification algorithms for real-time disease\n",
      "identification, benefiting small-scale farmers.\n",
      "• John Deere’s See & Spray technology [148] exemplifies\n",
      "object detection in precision agriculture. It leverages\n",
      "computer vision and AI to identify and precisely target\n",
      "individual weeds, reducing the need for indiscriminate\n",
      "herbicide application. This project illustrates how object\n",
      "detection algorithms optimize resource usage.\n",
      "• Blue River Technology [149] employs computer vision\n",
      "and machine learning for their See & Spray system,\n",
      "114368 VOLUME 11, 2023\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "autonomously identifying and selectively spraying her-\n",
      "bicides only where needed. This project demonstrates\n",
      "the practical implications of object detection algorithms\n",
      "in enhancing the sustainability and cost-effectiveness of\n",
      "crop management.\n",
      "• IBM’s Smart Agriculture solution [150] leverages AI\n",
      "and machine learning to offer insights into crop health\n",
      "and disease detection. By integrating data from various\n",
      "sources, including satellite imagery and IoT sensors,\n",
      "it provides actionable recommendations to farmers.\n",
      "This project showcases the integration of advanced\n",
      "algorithms with a data-driven approach in precision\n",
      "agriculture.\n",
      "VII. CHALLENGES IN PLANT DISEASE DETECTION\n",
      "After a detailed review of ML and DL algorithms for\n",
      "plant disease detection and classification and the detailed\n",
      "computational study on five state-of-the-art object detec-\n",
      "tion algorithms for plant disease detection and eighteen\n",
      "state-of-the-art classification algorithms for plant disease\n",
      "classification on a widely-used dataset, we have identified\n",
      "several challenges in practical applications of plant disease\n",
      "detection:\n",
      "1) There is a lack of models that handle non-image\n",
      "data. Most existing classification and object detection\n",
      "algorithms focus solely on image data, neglecting other\n",
      "relevant information such as temperature and humidity.\n",
      "Developing techniques to incorporate non-image data\n",
      "is essential for more accurate predictions.\n",
      "2) There are only a few completely annotated open\n",
      "datasets. Many studies rely on the PlantVillage dataset,\n",
      "which was obtained in a controlled laboratory setting.\n",
      "Generating larger datasets under real-world conditions\n",
      "is crucial. Collaborative efforts are needed to create\n",
      "representative datasets.\n",
      "3) Most works treat the disease detection problem as\n",
      "a classification problem, either binary classification\n",
      "or multi-class classification. While many works treat\n",
      "disease detection as a classification problem, more\n",
      "emphasis should be placed on object detection to\n",
      "identify both the disease type and affected regions in\n",
      "the image.\n",
      "4) Most papers use a single dataset used to train and test\n",
      "the model. Models trained on a single dataset often\n",
      "perform poorly on different datasets. It is essential to\n",
      "consider diverse datasets to improve model robustness.\n",
      "5) Overreliance on CNN architectures: While CNNs yield\n",
      "good results, exploring other neural network archi-\n",
      "tectures like recurrent neural networks can enhance\n",
      "disease detection methods.\n",
      "6) Small leaf and early-stage disease recognition: Current\n",
      "datasets mainly consist of images with large leaves.\n",
      "Annotating datasets for early-stage disease detection\n",
      "and small leaf recognition is necessary.\n",
      "7) Challenges with illumination and occlusion: Existing\n",
      "algorithms struggle with images under varying lighting\n",
      "conditions and occlusion. More robust methods are\n",
      "needed to address these issues.\n",
      "8) Computational efficiency: Many models are compu-\n",
      "tationally intensive, hindering real-time applications.\n",
      "Researchers should focus on improving the computa-\n",
      "tional efficiency of their models.\n",
      "VIII. FUTURE DIRECTIONS IN PLANT DISEASE DETECTION\n",
      "In addition to the challenges mentioned above, there are\n",
      "several promising directions for future research in plant\n",
      "disease detection:\n",
      "1) Integration of non-image data: Develop models that\n",
      "can effectively integrate non-image data, such as envi-\n",
      "ronmental factors, into disease detection algorithms to\n",
      "improve prediction accuracy.\n",
      "2) Creation of diverse and real-world datasets: Collab-\n",
      "orate with experts to generate large, representative\n",
      "datasets under real-world agricultural conditions to\n",
      "enhance the generalizability of models.\n",
      "3) Emphasis on object detection: Explore the potential of\n",
      "object detection methods for predicting plant diseases,\n",
      "which can provide more detailed information about\n",
      "disease localization.\n",
      "4) Robustness across datasets: Develop models that per-\n",
      "form consistently well across various datasets to ensure\n",
      "their practical utility.\n",
      "5) Exploration of alternative neural network architectures:\n",
      "Experiment with different neural network architectures\n",
      "beyond CNNs, such as recurrent neural networks,\n",
      "to uncover their potential in disease detection.\n",
      "6) Early-stage and small leaf recognition: Annotate\n",
      "datasets specifically for early-stage disease recognition\n",
      "and the identification of diseases on plants or leaves\n",
      "with small sizes.\n",
      "7) Addressing illumination and occlusion challenges:\n",
      "Implement techniques to enhance the robustness of\n",
      "algorithms in the presence of variable lighting condi-\n",
      "tions and occluded images.\n",
      "8) Improved computational efficiency: Focus on optimiz-\n",
      "ing model architectures and algorithms to make them\n",
      "suitable for real-time applications.\n",
      "IX. CONCLUSION\n",
      "The aim of this study is to examine existing research that\n",
      "utilizes ML and DL techniques in precision agriculture,\n",
      "with a particular focus on plant disease detection and\n",
      "classification methods. Additionally, a new classification\n",
      "scheme is introduced, which categorizes all relevant works\n",
      "into their respective classes. We separate the studies into\n",
      "two main categories depending on the methodology that they\n",
      "use (i.e. classification and object detection). Furthermore,\n",
      "we present the available datasets for plant disease detection\n",
      "and classification, and provide details about their classes\n",
      "and data, and whether the specific dataset is suitable for\n",
      "classification or object detection.\n",
      "VOLUME 11, 2023 114369\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "TABLE 7. Classification of the related works.\n",
      "114370 VOLUME 11, 2023\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "TABLE 7. (Continued.) Classification of the related works.\n",
      "VOLUME 11, 2023 114371\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "TABLE 7. (Continued.) Classification of the related works.\n",
      "114372 VOLUME 11, 2023\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "TABLE 7. (Continued.) Classification of the related works.\n",
      "In addition, we perform an extensive computational study\n",
      "on five state-of-the-art object detection algorithms on the\n",
      "PlantDoc dataset to detect the diseases on the leaves, and\n",
      "eighteen state-of-the-art classification algorithms on the\n",
      "PlantDoc dataset, to predict whether or not there is a disease\n",
      "in a leaf and which one it is. For the object detection problem,\n",
      "YOLOv5 is able to achieve a high accuracy not only at\n",
      "the strict AP metric of the COCO challenge but also in the\n",
      "detection of small, medium, and large objects in the images.\n",
      "For the classification problem, the networks ResNet50 and\n",
      "MobileNetv2 had the most optimal trade-off on accuracy and\n",
      "training time. ResNet50 achieved a total accuracy of 61.01%\n",
      "and was trained on about 18 minutes, while MobileNetv2\n",
      "achieved a total accuracy of 59.74% and was trained for about\n",
      "16 minutes.\n",
      "In future work, we plan to study more algorithms for\n",
      "classification and object detection on more datasets to see\n",
      "whether the results are consistent across different datasets.\n",
      "We also intend to study some image preprocessing and data\n",
      "augmentation techniques to see whether the accuracy of the\n",
      "algorithms can be improved with these techniques.\n",
      "APPENDIX\n",
      "CLASSIFICATION OF THE RELATED WORKS\n",
      "See Table 7.\n",
      "REFERENCES\n",
      "[1] Food and Agriculture Organization. (2019). New Standards\n",
      "to Curb the Global Spread of Plant Pests and Diseases.\n",
      "Accessed: Nov. 8, 2022. [Online]. Available: https://www.fao.org/\n",
      "news/story/en/item/1187738/icode/\n",
      "[2] Y . Mekonnen, S. Namuduri, L. Burton, A. Sarwat, and S. Bhansali,\n",
      "‘‘Machine learning techniques in wireless sensor network based pre-\n",
      "cision agriculture,’’ J. Electrochem. Soc., vol. 167, no. 3, Jan. 2020,\n",
      "Art. no. 037522.\n",
      "[3] L. Benos, A. C. Tagarakis, G. Dolias, R. Berruto, D. Kateris, and\n",
      "D. Bochtis, ‘‘Machine learning in agriculture: A comprehensive updated\n",
      "review,’’Sensors, vol. 21, no. 11, p. 3758, May 2021.\n",
      "[4] L. Li, S. Zhang, and B. Wang, ‘‘Plant disease detection and classification\n",
      "by deep learning—A review,’’ IEEE Access, vol. 9, pp. 56683–56698,\n",
      "2021.\n",
      "[5] J. Liu and X. Wang, ‘‘Plant diseases and pests detection based on deep\n",
      "learning: A review,’’Plant Methods, vol. 17, no. 1, pp. 1–18, Dec. 2021.\n",
      "[6] Y . Y uan, L. Chen, H. Wu, and L. Li, ‘‘Advanced agricultural disease image\n",
      "recognition technologies: A review,’’Inf. Process. Agricult., vol. 9, no. 1,\n",
      "pp. 48–59, Mar. 2022.\n",
      "[7] S. P . Mohanty, D. P . Hughes, and M. Salathé, ‘‘Using deep learning for\n",
      "image-based plant disease detection,’’Frontiers Plant Sci., vol. 7, p. 1419,\n",
      "Sep. 2016.\n",
      "[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‘‘ImageNet classification\n",
      "with deep convolutional neural networks,’’ in Proc. Adv. Neural Inf.\n",
      "Process. Syst., vol. 25, 2012, pp. 1097–1105.\n",
      "[9] C. Szegedy, W. Liu, Y . Jia, P . Sermanet, S. Reed, D. Anguelov, D. Erhan,\n",
      "V . V anhoucke, and A. Rabinovich, ‘‘Going deeper with convolutions,’’\n",
      "in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2015,\n",
      "pp. 1–9.\n",
      "[10] Kaggle. (2018). Plantvillage Dataset. Accessed: Nov. 8, 2022. [Online].\n",
      "Available: https://www.kaggle.com/datasets/emmarex/plantdisease\n",
      "[11] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\n",
      "S. Guadarrama, and T. Darrell, ‘‘Caffe: Convolutional architecture for\n",
      "fast feature embedding,’’ in Proc. 22nd ACM Int. Conf. Multimedia,\n",
      "Nov. 2014, pp. 675–678.\n",
      "[12] S. Sladojevic, M. Arsenovic, A. Anderla, D. Culibrk, and D. Stefanovic,\n",
      "‘‘Deep neural networks based recognition of plant diseases by leaf\n",
      "image classification,’’ Comput. Intell. Neurosci., vol. 2016, pp. 1–11,\n",
      "May 2016.\n",
      "[13] J. Amara, B. Bouaziz, and A. Algergawy, ‘‘A deep learning-based\n",
      "approach for banana leaf diseases classification,’’ in Proc. Datenbanksys.\n",
      "Für Bus., Technol. Web (BTW), Workshopband, CA, USA, Jul. 2017,\n",
      "pp. 1–24.\n",
      "VOLUME 11, 2023 114373\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "[14] Y . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,\n",
      "W. Hubbard, and L. D. Jackel, ‘‘Backpropagation applied to handwritten\n",
      "zip code recognition,’’ Neural Comput., vol. 1, no. 4, pp. 541–551,\n",
      "Dec. 1989.\n",
      "[15] M. Brahimi, K. Boukhalfa, and A. Moussaoui, ‘‘Deep learning for tomato\n",
      "diseases: Classification and symptoms visualization,’’ Appl. Artif. Intell.,\n",
      "vol. 31, no. 4, pp. 299–315, Apr. 2017.\n",
      "[16] A. C. Cruz, A. Luvisi, L. De Bellis, and Y . Ampatzidis,\n",
      "‘‘X-FIDO: An effective application for detecting olive quick decline\n",
      "syndrome with deep learning and data fusion,’’ Frontiers Plant Sci.,\n",
      "vol. 8, p. 1741, Oct. 2017.\n",
      "[17] C. DeChant, T. Wiesner-Hanks, S. Chen, E. L. Stewart, J. Y osinski,\n",
      "M. A. Gore, R. J. Nelson, and H. Lipson, ‘‘Automated identification of\n",
      "northern leaf blight-infected maize plants from field imagery using deep\n",
      "learning,’’ Phytopathology, vol. 107, no. 11, pp. 1426–1432, Nov. 2017.\n",
      "[18] B. Liu, Y . Zhang, D. He, and Y . Li, ‘‘Identification of apple leaf diseases\n",
      "based on deep convolutional neural networks,’’ Symmetry, vol. 10, no. 1,\n",
      "p. 11, Dec. 2017.\n",
      "[19] Y . Lu, S. Yi, N. Zeng, Y . Liu, and Y . Zhang, ‘‘Identification of rice diseases\n",
      "using deep convolutional neural networks,’’ Neurocomputing, vol. 267,\n",
      "pp. 378–384, Dec. 2017.\n",
      "[20] D. Oppenheim and G. Shani, ‘‘Potato disease classification using\n",
      "convolution neural networks,’’ Adv. Animal Biosci., vol. 8, no. 2,\n",
      "pp. 244–249, 2017.\n",
      "[21] K. Simonyan and A. Zisserman, ‘‘V ery deep convolutional networks for\n",
      "large-scale image recognition,’’ 2014, arXiv:1409.1556.\n",
      "[22] G. Wang, Y . Sun, and J. Wang, ‘‘Automatic image-based plant disease\n",
      "severity estimation using deep learning,’’ Comput. Intell. Neurosci.,\n",
      "vol. 2017, pp. 1–8, Jul. 2017.\n",
      "[23] C. Szegedy, V . V anhoucke, S. Ioffe, J. Shlens, and Z. Wojna, ‘‘Rethinking\n",
      "the inception architecture for computer vision,’’ in Proc. IEEE Conf.\n",
      "Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 2818–2826.\n",
      "[24] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for\n",
      "image recognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n",
      "(CVPR), Jun. 2016, pp. 770–778.\n",
      "[25] J. Abdulridha, Y . Ampatzidis, R. Ehsani, and A. I. de Castro, ‘‘Evaluating\n",
      "the performance of spectral features and multivariate analysis tools to\n",
      "detect laurel wilt disease and nutritional deficiency in avocado,’’Comput.\n",
      "Electron. Agricult., vol. 155, pp. 203–211, Dec. 2018.\n",
      "[26] J. G. A. Barbedo, ‘‘Factors influencing the use of deep learning\n",
      "for plant disease recognition,’’ Biosyst. Eng., vol. 172, pp. 84–91,\n",
      "Aug. 2018.\n",
      "[27] J. G. A. Barbedo, L. V . Koenigkan, B. A. Halfeld-Vieira, R. V . Costa,\n",
      "K. L. Nechet, C. V . Godoy, M. L. Junior, F. R. A. Patricio, V . Talamini,\n",
      "L. G. Chitarra, S. A. S. Oliveira, A. K. N. Ishida, J. M. C. Fernandes,\n",
      "T. T. Santos, F. R. Cavalcanti, D. Terao, and F. Angelotti, ‘‘Annotated\n",
      "plant pathology databases for image-based detection and recognition\n",
      "of diseases,’’ IEEE Latin Amer . Trans., vol. 16, no. 6, pp. 1749–1757,\n",
      "Jun. 2018.\n",
      "[28] K. P . Ferentinos, ‘‘Deep learning models for plant disease detection\n",
      "and diagnosis,’’ Comput. Electron. Agricult., vol. 145, pp. 311–318,\n",
      "Feb. 2018.\n",
      "[29] A. Krizhevsky, ‘‘One weird trick for parallelizing convolutional neural\n",
      "networks,’’ 2014, arXiv:1404.5997.\n",
      "[30] P . Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y . LeCun,\n",
      "‘‘OverFeat: Integrated recognition, localization and detection using\n",
      "convolutional networks,’’ 2013, arXiv:1312.6229.\n",
      "[31] J. Lu, R. Ehsani, Y . Shi, A. I. de Castro, and S. Wang, ‘‘Detection of multi-\n",
      "tomato leaf diseases (late blight, target and bacterial spots) in different\n",
      "stages by using a spectral-based sensor,’’Sci. Rep., vol. 8, no. 1, pp. 1–11,\n",
      "Feb. 2018.\n",
      "[32] A. F. Fuentes, S. Y oon, J. Lee, and D. S. Park, ‘‘High-performance\n",
      "deep neural network-based tomato plant diseases and pests diagnosis\n",
      "system with refinement filter bank,’’ Frontiers Plant Sci., vol. 9, p. 1162,\n",
      "Aug. 2018.\n",
      "[33] M. Kerkech, A. Hafiane, and R. Canals, ‘‘Deep leaning approach with\n",
      "colorimetric spaces and vegetation indices for vine diseases detection\n",
      "in UA V images,’’ Comput. Electron. Agricult., vol. 155, pp. 237–243,\n",
      "Dec. 2018.\n",
      "[34] M. Pineda, M. L. Pérez-Bueno, and M. Barón, ‘‘Detection of bacterial\n",
      "infection in melon plants by classification methods based on imaging\n",
      "data,’’ Frontiers Plant Sci., vol. 9, p. 164, Feb. 2018.\n",
      "[35] M. Sharif, M. A. Khan, Z. Iqbal, M. F. Azam, M. I. U. Lali, and\n",
      "M. Y . Javed, ‘‘Detection and classification of citrus diseases in agriculture\n",
      "based on optimized weighted segmentation and feature selection,’’\n",
      "Comput. Electron. Agricult., vol. 150, pp. 220–234, Jul. 2018.\n",
      "[36] USDA/APHIS/PPQ Center for Plant Health Science and Technology.\n",
      "(2010). Citrus Diseases Database. Accessed: Nov. 8, 2022. [Online].\n",
      "Available: http://idtools.org/id/citrus/diseases/gallery.php\n",
      "[37] X. Zhang, Y . Qiao, F. Meng, C. Fan, and M. Zhang, ‘‘Identification of\n",
      "maize leaf diseases using improved deep convolutional neural networks,’’\n",
      "IEEE Access, vol. 6, pp. 30370–30377, 2018.\n",
      "[38] A. Krizhevsky and G. Hinton, ‘‘Learning multiple layers of features from\n",
      "tiny images,’’ M.S. thesis, Dept. Comput. Sci., Univ. Toronto, Toronto,\n",
      "ON, Canada, 2009.\n",
      "[39] J. Abdulridha, R. Ehsani, A. Abd-Elrahman, and Y . Ampatzidis,\n",
      "‘‘A remote sensing technique for detecting laurel wilt disease in avocado\n",
      "in presence of other biotic and abiotic stresses,’’ Comput. Electron.\n",
      "Agricult., vol. 156, pp. 549–557, Jan. 2019.\n",
      "[40] H. Al-Saddik, J. C. Simon, and F. Cointault, ‘‘Assessment of the optimal\n",
      "spectral bands for designing a sensor for vineyard disease detection:\n",
      "The case of ‘Flavescence dorée,’’’ Precis. Agricult., vol. 20, no. 2,\n",
      "pp. 398–422, Apr. 2019.\n",
      "[41] M. Arsenovic, M. Karanovic, S. Sladojevic, A. Anderla, and\n",
      "D. Stefanovic, ‘‘Solving current limitations of deep learning based\n",
      "approaches for plant disease detection,’’ Symmetry, vol. 11, no. 7, p. 939,\n",
      "Jul. 2019.\n",
      "[42] J. G. A. Barbedo, ‘‘Plant disease identification from individual lesions\n",
      "and spots using deep learning,’’ Biosyst. Eng., vol. 180, pp. 96–107,\n",
      "Apr. 2019.\n",
      "[43] S. Coulibaly, B. Kamsu-Foguem, D. Kamissoko, and D. Traore, ‘‘Deep\n",
      "neural networks with transfer learning in millet crop images,’’ Comput.\n",
      "Ind., vol. 108, pp. 115–120, Jun. 2019.\n",
      "[44] G. Dhingra, V . Kumar, and H. D. Joshi, ‘‘A novel computer vision based\n",
      "neutrosophic approach for leaf disease identification and classification,’’\n",
      "Measurement, vol. 135, pp. 782–794, Mar. 2019.\n",
      "[45] G. Hu, H. Wu, Y . Zhang, and M. Wan, ‘‘A low shot learning method for\n",
      "tea leaf’s disease identification,’’ Comput. Electron. Agricult., vol. 163,\n",
      "Aug. 2019, Art. no. 104852.\n",
      "[46] A. Radford, L. Metz, and S. Chintala, ‘‘Unsupervised representation\n",
      "learning with deep convolutional generative adversarial networks,’’ 2015,\n",
      "arXiv:1511.06434.\n",
      "[47] H. Huang, J. Deng, Y . Lan, A. Y ang, L. Zhang, S. Wen, H. Zhang,\n",
      "Y . Zhang, and Y . Deng, ‘‘Detection of helminthosporium leaf blotch\n",
      "disease based on UA V imagery,’’ Appl. Sci., vol. 9, no. 3, p. 558,\n",
      "Feb. 2019.\n",
      "[48] J. Abdulridha, Y . Ampatzidis, S. C. Kakarla, and P . Roberts, ‘‘Detection\n",
      "of target spot and bacterial spot diseases in tomato using UA V -based\n",
      "and benchtop-based hyperspectral imaging techniques,’’Precis. Agricult.,\n",
      "vol. 21, no. 5, pp. 955–978, Oct. 2020.\n",
      "[49] J. Abdulridha, Y . Ampatzidis, J. Qureshi, and P . Roberts, ‘‘Laboratory and\n",
      "UA V -based identification and classification of tomato yellow leaf curl,\n",
      "bacterial spot, and target spot diseases in tomato utilizing hyperspectral\n",
      "imaging and machine learning,’’ Remote Sens., vol. 12, no. 17, p. 2732,\n",
      "Aug. 2020.\n",
      "[50] J. Abdulridha, Y . Ampatzidis, P . Roberts, and S. C. Kakarla, ‘‘Detecting\n",
      "powdery mildew disease in squash at different stages using UA V -based\n",
      "hyperspectral imaging and artificial intelligence,’’Biosyst. Eng., vol. 197,\n",
      "pp. 135–148, Sep. 2020.\n",
      "[51] M. Agarwal, S. K. Gupta, and K. K. Biswas, ‘‘Development of efficient\n",
      "CNN model for tomato crop disease identification,’’ Sustain. Comput.,\n",
      "Informat. Syst., vol. 28, Dec. 2020, Art. no. 100407.\n",
      "[52] A. Anagnostis, G. Asiminari, E. Papageorgiou, and D. Bochtis, ‘‘A\n",
      "convolutional neural networks based method for anthracnose infected\n",
      "walnut tree leaves identification,’’ Appl. Sci., vol. 10, no. 2, p. 469,\n",
      "Jan. 2020.\n",
      "[53] G. Huang, Z. Liu, L. V an Der Maaten, and K. Q. Weinberger, ‘‘Densely\n",
      "connected convolutional networks,’’ in Proc. IEEE Conf. Comput. Vis.\n",
      "Pattern Recognit. (CVPR), Jul. 2017, pp. 2261–2269.\n",
      "[54] M. Chen, F. Brun, M. Raynal, and D. Makowski, ‘‘Forecasting severe\n",
      "grape downy mildew attacks using machine learning,’’ PLoS ONE,\n",
      "vol. 15, no. 3, Mar. 2020, Art. no. e0230254.\n",
      "[55] R. Cristin, B. S. Kumar, C. Priya, and K. Karthick, ‘‘Deep neural network\n",
      "based rider-cuckoo search algorithm for plant disease detection,’’ Artif.\n",
      "Intell. Rev., vol. 53, no. 7, pp. 4993–5018, Oct. 2020.\n",
      "114374 VOLUME 11, 2023\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "[56] S. H. Lee, H. Goëau, P . Bonnet, and A. Joly, ‘‘New perspectives on\n",
      "plant disease characterization based on deep learning,’’Comput. Electron.\n",
      "Agricult., vol. 170, Mar. 2020, Art. no. 105220.\n",
      "[57] S. Ioffe and C. Szegedy, ‘‘Batch normalization: Accelerating deep\n",
      "network training by reducing internal covariate shift,’’ in Proc. Int. Conf.\n",
      "Mach. Learn., 2015, pp. 448–456.\n",
      "[58] S. Giraddi, S. Desai, and A. Deshpande, ‘‘Deep learning for agricultural\n",
      "plant disease detection,’’ in Proc. ICDSMLA. Singapore: Springer, 2020,\n",
      "pp. 864–871.\n",
      "[59] Y . Guo, J. Zhang, C. Yin, X. Hu, Y . Zou, Z. Xue, and W. Wang, ‘‘Plant\n",
      "disease identification based on deep learning algorithm in smart farming,’’\n",
      "Discrete Dyn. Nature Soc., vol. 2020, pp. 1–11, Aug. 2020.\n",
      "[60] S. Ren, K. He, R. Girshick, and J. Sun, ‘‘Faster R-CNN: Towards real-time\n",
      "object detection with region proposal networks,’’ inProc. Adv. Neural Inf.\n",
      "Process. Syst., vol. 28, 2015.\n",
      "[61] P . Getreuer, ‘‘Chan-vese segmentation,’’ Image Process. Line, vol. 2,\n",
      "pp. 214–224, Aug. 2012.\n",
      "[62] Institute of Botany of the Chinese Academy of Sciences. (2022). Plant\n",
      "Photo Bank of China. Accessed: Nov. 8, 2022. [Online]. Available:\n",
      "https://ppbc.iplant.cn/\n",
      "[63] M. T. Habib, A. Majumder, A. Z. M. Jakaria, M. Akter, M. S. Uddin, and\n",
      "F. Ahmed, ‘‘Machine vision based papaya disease recognition,’’ J. King\n",
      "Saud Univ.-Comput. Inf. Sci., vol. 32, no. 3, pp. 300–309, Mar. 2020.\n",
      "[64] K. Karadağ, M. E. Tenekeci, R. Taşaltın, and A. Bilgili, ‘‘Detection\n",
      "of pepper fusarium disease using machine learning algorithms based\n",
      "on spectral reflectance,’’ Sustain. Computing: Informat. Syst., vol. 28,\n",
      "p. 100299, 2020.\n",
      "[65] A. Karlekar and A. Seal, ‘‘SoyNet: Soybean leaf diseases classification,’’\n",
      "Comput. Electron. Agricult., vol. 172, May 2020, Art. no. 105342.\n",
      "[66] R. Karthik, M. Hariharan, S. Anand, P . Mathikshara, A. Johnson,\n",
      "and R. Menaka, ‘‘Attention embedded residual CNN for disease\n",
      "detection in tomato leaves,’’ Appl. Soft Comput., vol. 86, Jan. 2020,\n",
      "Art. no. 105933.\n",
      "[67] M. Kerkech, A. Hafiane, and R. Canals, ‘‘Vine disease detection in\n",
      "UA V multispectral images using optimized image registration and deep\n",
      "learning segmentation approach,’’ Comput. Electron. Agricult., vol. 174,\n",
      "Jul. 2020, Art. no. 105446.\n",
      "[68] E. Khalili, S. Kouchaki, S. Ramazi, and F. Ghanati, ‘‘Machine learning\n",
      "techniques for soybean charcoal rot disease prediction,’’ Frontiers Plant\n",
      "Sci., vol. 11, Dec. 2020, Art. no. 590529.\n",
      "[69] E. Khalili. (2020). Soybean Charcoal Rot Disease Prediction Dataset.\n",
      "Accessed: Nov. 8, 2022. [Online]. Available: https://github.com/Elham-\n",
      "khalili/Soybean-Charcoal-Rot-Disease-Prediction-Dataset-code\n",
      "[70] S. Ramesh and D. Vydeki, ‘‘Recognition and classification of paddy leaf\n",
      "diseases using optimized deep neural network with Jaya algorithm,’’ Inf.\n",
      "Process. Agricult., vol. 7, no. 2, pp. 249–260, Jun. 2020.\n",
      "[71] S. V erma, A. Chug, and A. P . Singh, ‘‘Application of convolutional neural\n",
      "networks for evaluation of disease severity in tomato plant,’’ J. Discrete\n",
      "Math. Sci. Cryptogr ., vol. 23, no. 1, pp. 273–282, Jan. 2020.\n",
      "[72] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,\n",
      "and K. Keutzer, ‘‘SqueezeNet: Alexnet-level accuracy with 50x fewer\n",
      "parameters and <0.5 MB model size,’’ 2016, arXiv:1602.07360.\n",
      "[73] D. V elásquez, A. Sánchez, S. Sarmiento, M. Toro, M. Maiza, and\n",
      "B. Sierra, ‘‘A method for detecting coffee leaf rust through wireless\n",
      "sensor networks, remote sensing, and deep learning: Case study of\n",
      "the caturra variety in Colombia,’’ Appl. Sci., vol. 10, no. 2, p. 697,\n",
      "Jan. 2020.\n",
      "[74] Q. Y an, B. Y ang, W. Wang, B. Wang, P . Chen, and J. Zhang, ‘‘Apple\n",
      "leaf diseases recognition based on an improved convolutional neural\n",
      "network,’’Sensors, vol. 20, no. 12, p. 3535, Jun. 2020.\n",
      "[75] E&M AI Lab. (2022). Global AI Challenge for Building\n",
      "E&M Facilities. Accessed: Nov. 8, 2022. [Online]. Available:\n",
      "https://www.globalaichallenge.com/en/home\n",
      "[76] Z. Zhang, P . Flores, C. Igathinathane, D. L. Naik, R. Kiran, and\n",
      "J. K. Ransom, ‘‘Wheat lodging detection from UAS imagery using\n",
      "machine learning algorithms,’’ Remote Sens., vol. 12, no. 11, p. 1838,\n",
      "Jun. 2020.\n",
      "[77] A. Abbas, S. Jain, M. Gour, and S. V ankudothu, ‘‘Tomato plant\n",
      "disease detection using transfer learning with C-GAN synthetic images,’’\n",
      "Comput. Electron. Agricult., vol. 187, Aug. 2021, Art. no. 106279.\n",
      "[78] M. Mirza and S. Osindero, ‘‘Conditional generative adversarial nets,’’\n",
      "2014, arXiv:1411.1784.\n",
      "[79] E. Akanksha, N. Sharma, and K. Gulati, ‘‘OPNN: Optimized probabilistic\n",
      "neural network based automatic detection of maize plant disease\n",
      "detection,’’ in Proc. 6th Int. Conf. Inventive Comput. Technol. (ICICT),\n",
      "Jan. 2021, pp. 1322–1328.\n",
      "[80] P . Bedi and P . Gole, ‘‘Plant disease detection using hybrid model based\n",
      "on convolutional autoencoder and convolutional neural network,’’ Artif.\n",
      "Intell. Agricult., vol. 5, pp. 90–101, Jan. 2021.\n",
      "[81] M. E. H. Chowdhury, T. Rahman, A. Khandakar, M. A. Ayari, A.\n",
      "U. Khan, M. S. Khan, N. Al-Emadi, M. B. I. Reaz, M. T. Islam,\n",
      "and S. H. M. Ali, ‘‘Automatic and reliable leaf disease detection using\n",
      "deep learning techniques,’’ AgriEngineering, vol. 3, no. 2, pp. 294–312,\n",
      "May 2021.\n",
      "[82] M. Tan and Q. Le, ‘‘EfficientNet: Rethinking model scaling for\n",
      "convolutional neural networks,’’ in Proc. Int. Conf. Mach. Learn., 2019,\n",
      "pp. 6105–6114.\n",
      "[83] O. Ronneberger, P . Fischer, and T. Brox, ‘‘U-Net: Convolutional networks\n",
      "for biomedical image segmentation,’’ in Proc. Int. Conf. Med. Image\n",
      "Comput. Comput.-Assist. Intervent. Cham, Switzerland: Springer, 2015,\n",
      "pp. 234–241.\n",
      "[84] V . Petsiuk. (2019). Lung-Segmentation-2D: Ng Fields Segmentation\n",
      "on CXR Images Using Convotional Neural Networks.\n",
      "Accessed: Nov. 8, 2022. [Online]. Available: https://github.com/imlab-\n",
      "uiip/lung-segmentation-2d\n",
      "[85] R. Dwivedi, S. Dey, C. Chakraborty, and S. Tiwari, ‘‘Grape disease\n",
      "detection network based on multi-task learning and attention features,’’\n",
      "IEEE Sensors J., vol. 21, no. 16, pp. 17573–17580, Aug. 2021.\n",
      "[86] M. Mishra, P . Choudhury, and B. Pati, ‘‘Modified ride-NN optimizer\n",
      "for the IoT based plant disease detection,’’ J. Ambient Intell. Humanized\n",
      "Comput., vol. 12, no. 1, pp. 691–703, Jan. 2021.\n",
      "[87] A. M. Mostafa, S. A. Kumar, T. Meraj, H. T. Rauf, A. A. Alnuaim,\n",
      "and M. A. Alkhayyal, ‘‘Guava disease detection using deep convolutional\n",
      "neural networks: A case study of guava plants,’’ Appl. Sci., vol. 12, no. 1,\n",
      "p. 239, Dec. 2021.\n",
      "[88] B. V . Patil and P . S. Patil, ‘‘Computational method for cotton plant\n",
      "disease detection of crop management using deep learning and Internet of\n",
      "Things platforms,’’ in Evolutionary Computing and Mobile Sustainable\n",
      "Networks. Singapore: Springer, 2021, pp. 875–885.\n",
      "[89] G. Sambasivam and G. D. Opiyo, ‘‘A predictive machine learning\n",
      "application in agriculture: Cassava disease detection and classification\n",
      "with imbalanced dataset using convolutional neural networks,’’ Egyptian\n",
      "Informat. J., vol. 22, no. 1, pp. 27–34, Mar. 2021.\n",
      "[90] Kaggle. (2019). Cassava Disease Classification. Accessed:\n",
      "Nov. 8, 2022. [Online]. Available: https://www.kaggle.com/c/cassava-\n",
      "disease/overview\n",
      "[91] R. Sujatha, J. M. Chatterjee, N. Jhanjhi, and S. N. Brohi, ‘‘Performance\n",
      "of deep learning vs machine learning in plant leaf disease detection,’’\n",
      "Microprocessors Microsyst., vol. 80, Feb. 2021, Art. no. 103615.\n",
      "[92] H. T. Rauf, B. A. Saleem, M. I. U. Lali, M. A. Khan, M. Sharif, and\n",
      "S. A. C. Bukhari, ‘‘A citrus fruits and leaves dataset for detection and\n",
      "classification of citrus diseases through machine learning,’’ Data Brief,\n",
      "vol. 26, Oct. 2019, Art. no. 104340.\n",
      "[93] V . Tiwari, R. C. Joshi, and M. K. Dutta, ‘‘Dense convolutional neural\n",
      "networks based multiclass plant disease detection and classification using\n",
      "leaf images,’’ Ecol. Informat., vol. 63, Jul. 2021, Art. no. 101289.\n",
      "[94] AIR Lab Makerere University. (2020). iBean Leaf Image Dataset.\n",
      "Accessed: Nov. 8, 2022. [Online]. Available: https://github.com/AI-\n",
      "Lab-Makerere/ibean and https://www.fao.org/news/story/en/item/\n",
      "1187738/icode/\n",
      "[95] H. B. Prajapati, J. P . Shah, and V . K. Dabhi, ‘‘Detection and classification\n",
      "of rice plant diseases,’’Intell. Decis. Technol., vol. 11, no. 3, pp. 357–373,\n",
      "Aug. 2017.\n",
      "[96] Y . Zhao, Z. Chen, X. Gao, W. Song, Q. Xiong, J. Hu, and Z. Zhang,\n",
      "‘‘Plant disease detection using generated leaves based on DoubleGAN,’’\n",
      "IEEE/ACM Trans. Comput. Biol. Bioinf. , vol. 19, no. 3, pp. 1817–1826,\n",
      "May/Jun. 2021.\n",
      "[97] V . Kathole and M. Munot, ‘‘Deep learning models for tomato plant\n",
      "disease detection,’’ in Advanced Machine Intelligence and Signal\n",
      "Processing. Singapore: Springer, 2022, pp. 679–686.\n",
      "[98] S.-Q. Pan, J.-F. Qiao, R. Wang, H.-L. Y u, C. Wang, K. Taylor, and\n",
      "H.-Y . Pan, ‘‘Intelligent diagnosis of northern corn leaf blight with deep\n",
      "learning model,’’ J. Integrative Agricult., vol. 21, no. 4, pp. 1094–1105,\n",
      "Apr. 2022.\n",
      "VOLUME 11, 2023 114375\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "[99] D. Shah, V . Trivedi, V . Sheth, A. Shah, and U. Chauhan, ‘‘ResTS:\n",
      "Residual deep interpretable architecture for plant disease detection,’’ Inf.\n",
      "Process. Agricult., vol. 9, no. 2, pp. 212–223, Jun. 2022.\n",
      "[100] M. Turkoglu, B. Y anikoğlu, and D. Hanbay, ‘‘Plantdiseasenet: Convo-\n",
      "lutional neural network ensemble for plant disease and pest detection,’’\n",
      "Signal, Image Video Process., vol. 16, no. 2, pp. 301–309, 2022.\n",
      "[101] M. Turkoglu. (2021). Turkey-Plantdataset. Accessed: Nov. 8, 2022.\n",
      "[Online]. Available: https://github.com/mturkoglu23/PlantDiseaseNet\n",
      "[102] S. V allabhajosyula, V . Sistla, and V . K. K. Kolli, ‘‘Transfer learning-based\n",
      "deep ensemble neural network for plant leaf disease detection,’’ J. Plant\n",
      "Diseases Protection, vol. 129, no. 3, pp. 545–558, Jun. 2022.\n",
      "[103] A. Howard, M. Sandler, B. Chen, W. Wang, L.-C. Chen, M. Tan, G. Chu,\n",
      "V . V asudevan, Y . Zhu, R. Pang, H. Adam, and Q. Le, ‘‘Searching for\n",
      "MobileNetV3,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),\n",
      "Oct. 2019, pp. 1314–1324.\n",
      "[104] B. Zoph, V . V asudevan, J. Shlens, and Q. V . Le, ‘‘Learning transferable\n",
      "architectures for scalable image recognition,’’ in Proc. IEEE/CVF Conf.\n",
      "Comput. Vis. Pattern Recognit., Jun. 2018, pp. 8697–8710.\n",
      "[105] A. Fuentes, S. Y oon, S. Kim, and D. Park, ‘‘A robust deep-learning-\n",
      "based detector for real-time tomato plant diseases and pests recognition,’’\n",
      "Sensors, vol. 17, no. 9, p. 2022, Sep. 2017.\n",
      "[106] J. Dai, Y . Li, K. He, and J. Sun, ‘‘R-FCN: Object detection via region-\n",
      "based fully convolutional networks,’’ in Proc. Adv. Neural Inf. Process.\n",
      "Syst., vol. 29, 2016, pp. 379–387.\n",
      "[107] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and\n",
      "A. C. Berg, ‘‘SSD: Single shot multibox detector,’’ in Proc. Eur . Conf.\n",
      "Comput. Vis. Cham, Switzerland: Springer, 2016, pp. 21–37.\n",
      "[108] P . Jiang, Y . Chen, B. Liu, D. He, and C. Liang, ‘‘Real-time detection\n",
      "of apple leaf diseases using deep learning approach based on improved\n",
      "convolutional neural networks,’’ IEEE Access, vol. 7, pp. 59069–59080,\n",
      "2019.\n",
      "[109] W.-S. Kim, D.-H. Lee, and Y .-J. Kim, ‘‘Machine vision-based automatic\n",
      "disease symptom detection of onion downy mildew,’’ Comput. Electron.\n",
      "Agricult., vol. 168, Jan. 2020, Art. no. 105099.\n",
      "[110] D. Li, R. Wang, C. Xie, L. Liu, J. Zhang, R. Li, F. Wang, M. Zhou, and\n",
      "W. Liu, ‘‘A recognition method for rice plant diseases and pests video\n",
      "detection based on deep convolutional neural network,’’ Sensors, vol. 20,\n",
      "no. 3, p. 578, Jan. 2020.\n",
      "[111] J. Redmon and A. Farhadi, ‘‘YOLOv3: An incremental improvement,’’\n",
      "2018, arXiv:1804.02767.\n",
      "[112] M. H. Saleem, S. Khanchi, J. Potgieter, and K. M. Arif, ‘‘Image-based\n",
      "plant disease identification by deep learning meta-architectures,’’ Plants,\n",
      "vol. 9, no. 11, p. 1451, Oct. 2020.\n",
      "[113] C. Szegedy, S. Ioffe, V . V anhoucke, and A. A. Alemi, ‘‘Inception-v4,\n",
      "inception-resnet and the impact of residual connections on learning,’’ in\n",
      "Proc. 31st AAAI Conf. Artif. Intell., 2017, pp. 4278–4284.\n",
      "[114] T. Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan, P . Dollár,\n",
      "and C. L. Zitnick, ‘‘Microsoft COCO: Common objects in context,’’ in\n",
      "Proc. Eur . Conf. Comput. Vis.Springer, 2014, pp. 740–755.\n",
      "[115] J. Sun, Y . Y ang, X. He, and X. Wu, ‘‘Northern maize leaf blight detection\n",
      "under complex field environment based on deep learning,’’ IEEE Access,\n",
      "vol. 8, pp. 33679–33688, 2020.\n",
      "[116] X. Xie, Y . Ma, B. Liu, J. He, S. Li, and H. Wang, ‘‘A deep-learning-based\n",
      "real-time detector for grape leaf diseases using improved convolutional\n",
      "neural networks,’’ Frontiers Plant Sci., vol. 11, p. 751, Jun. 2020.\n",
      "[117] C. Szegedy, W. Liu, Y . Jia, P . Sermanet, S. Reed, D. Anguelov, D. Erhan,\n",
      "V . V anhoucke, and A. Rabinovich, ‘‘Going deeper with convolutions,’’\n",
      "in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2015,\n",
      "pp. 1–9.\n",
      "[118] J. Hu, L. Shen, and G. Sun, ‘‘Squeeze-and-excitation networks,’’ in\n",
      "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018,\n",
      "pp. 7132–7141.\n",
      "[119] Y . Zhang, C. Song, and D. Zhang, ‘‘Deep learning-based object detection\n",
      "improvement for tomato disease,’’IEEE Access, vol. 8, pp. 56607–56614,\n",
      "2020.\n",
      "[120] A. M. Roy and J. Bhaduri, ‘‘A deep learning enabled multi-class plant\n",
      "disease detection model based on computer vision,’’ AI, vol. 2, no. 3,\n",
      "pp. 413–428, Aug. 2021.\n",
      "[121] M. G. Selvaraj, A. V ergara, F. Montenegro, H. A. Ruiz, N. Safari,\n",
      "D. Raymaekers, W. Ocimati, J. Ntamwira, L. Tits, A. B. Omondi, and\n",
      "G. Blomme, ‘‘Detection of banana plants and their major diseases through\n",
      "aerial images and machine learning methods: A case study in DR Congo\n",
      "and republic of Benin,’’ ISPRS J. Photogramm. Remote Sens., vol. 169,\n",
      "pp. 110–124, Nov. 2020.\n",
      "[122] J. Wang, L. Y u, J. Y ang, and H. Dong, ‘‘DBA_SSD: A novel end-\n",
      "to-end object detection algorithm applied to plant disease detection,’’\n",
      "Information, vol. 12, no. 11, p. 474, Nov. 2021.\n",
      "[123] A. Bochkovskiy, C.-Y . Wang, and H.-Y . M. Liao, ‘‘YOLOv4: Optimal\n",
      "speed and accuracy of object detection,’’ 2020, arXiv:2004.10934.\n",
      "[124] Z. Chen, R. Wu, Y . Lin, C. Li, S. Chen, Z. Y uan, S. Chen, and\n",
      "X. Zou, ‘‘Plant disease recognition model based on improved YOLOv5,’’\n",
      "Agronomy, vol. 12, no. 2, p. 365, Jan. 2022.\n",
      "[125] A. M. Roy, R. Bose, and J. Bhaduri, ‘‘A fast accurate fine-grain object\n",
      "detection model based on YOLOv4 deep neural network,’’ Neural\n",
      "Comput. Appl., vol. 34, no. 5, pp. 3895–3921, Mar. 2022.\n",
      "[126] H. Wang, S. Shang, D. Wang, X. He, K. Feng, and H. Zhu, ‘‘Plant\n",
      "disease detection and classification method based on the optimized\n",
      "lightweight YOLOv5 model,’’ Agriculture, vol. 12, no. 7, p. 931,\n",
      "Jun. 2022.\n",
      "[127] S. Siddharth, U. Singh, A. Kaul, and S. Jain, ‘‘A database of\n",
      "leaf images: Practice towards plant conservation with plant\n",
      "pathology,’’ Mendeley Data, 2019. [Online]. Available: https://data.\n",
      "mendeley.com/datasets/hb74ynkjcn/1\n",
      "[128] V . P . Kour and S. Arora, ‘‘Plantaek: A leaf database of native plants of\n",
      "jammu and kashmir,’’ in Recent Innovations in Computing. Singapore:\n",
      "Springer, 2022, pp. 359–368.\n",
      "[129] R. Thapa, K. Zhang, N. Snavely, S. Belongie, and A. Khan, ‘‘The plant\n",
      "pathology challenge 2020 data set to classify foliar disease of apples,’’\n",
      "Appl. Plant Sci., vol. 8, no. 9, p. e11390, Sep. 2020.\n",
      "[130] Science Data Bank. (2019). Corn Northern Leaf Blight Dataset.\n",
      "Accessed: Nov. 8, 2022. [Online]. Available: https://www.scidb.cn/en/\n",
      "detail?dataSetId=557575357876666368&dataSetType=project&code\n",
      "=p00001&tID=journalOne\n",
      "[131] D. Singh, N. Jain, P . Jain, P . Kayal, S. Kumawat, and N. Batra, ‘‘PlantDoc:\n",
      "A dataset for visual plant disease detection,’’ in Proc. 7th ACM IKDD\n",
      "CoDS 25th COMAD, 2020, pp. 249–253.\n",
      "[132] Y .-Y . Zheng, J.-L. Kong, X.-B. Jin, X.-Y . Wang, and M. Zuo, ‘‘CropDeep:\n",
      "The crop vision dataset for deep-learning-based classification and\n",
      "detection in precision agriculture,’’ Sensors, vol. 19, no. 5, p. 1058,\n",
      "Mar. 2019.\n",
      "[133] H. Wu, T. Wiesner-Hanks, E. L. Stewart, C. DeChant, N. Kaczmar,\n",
      "M. A. Gore, R. J. Nelson, and H. Lipson, ‘‘Autonomous detection of plant\n",
      "disease symptoms directly from aerial imagery,’’Plant Phenome J., vol. 2,\n",
      "no. 1, pp. 1–9, Jan. 2019.\n",
      "[134] J. Howard and S. Gugger, ‘‘Fastai: A layered API for deep learning,’’\n",
      "Information, vol. 11, no. 2, p. 108, Feb. 2020.\n",
      "[135] L. N. Smith, ‘‘A disciplined approach to neural network hyper-\n",
      "parameters: Part 1—Learning rate, batch size, momentum, and weight\n",
      "decay,’’ 2018,arXiv:1803.09820.\n",
      "[136] M. Tan, R. Pang, and Q. V . Le, ‘‘EfficientDet: Scalable and efficient object\n",
      "detection,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n",
      "(CVPR), Jun. 2020, pp. 10778–10787.\n",
      "[137] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‘‘ImageNet:\n",
      "A large-scale hierarchical image database,’’ inProc. IEEE Conf. Comput.\n",
      "Vis. Pattern Recognit., Jun. 2009, pp. 248–255.\n",
      "[138] T.-Y . Lin, P . Goyal, R. Girshick, K. He, and P . Dollár, ‘‘Focal loss for\n",
      "dense object detection,’’ in Proc. IEEE Int. Conf. Comput. Vis. (ICCV),\n",
      "Oct. 2017, pp. 2999–3007.\n",
      "[139] T.-Y . Lin, P . Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,\n",
      "‘‘Feature pyramid networks for object detection,’’ in Proc. IEEE Conf.\n",
      "Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 936–944.\n",
      "[140] G. Jocher. (2022). Ultralytics/YOLOv5: V3.1—Bug Fixes and Perfor-\n",
      "mance Improvements. Accessed: Nov. 8, 2022. [Online]. Available:\n",
      "https://github.com/ultralytics/yolov5\n",
      "[141] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, ‘‘Y ou only look once:\n",
      "Unified, real-time object detection,’’ in Proc. IEEE Conf. Comput. Vis.\n",
      "Pattern Recognit. (CVPR), Jun. 2016, pp. 779–788.\n",
      "[142] S. Ruder, ‘‘An overview of gradient descent optimization algorithms,’’\n",
      "2016, arXiv:1609.04747.\n",
      "[143] D. P . Kingma and J. Ba, ‘‘Adam: A method for stochastic optimization,’’\n",
      "2014, arXiv:1412.6980.\n",
      "[144] Ultralytics. Architecture Summary. Accessed: Jul. 1, 2023. [Online].\n",
      "Available: https://docs.ultralytics.com/yolov5/tutorials/architecture_\n",
      "description/\n",
      "[145] S. Xie, R. Girshick, P . Dollár, Z. Tu, and K. He, ‘‘Aggregated residual\n",
      "transformations for deep neural networks,’’ 2016, arXiv:1611.05431.\n",
      "[146] X. Zhang, X. Zhou, M. Lin, and J. Sun, ‘‘ShuffleNet: An extremely\n",
      "efficient convolutional neural network for mobile devices,’’ in\n",
      "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018,\n",
      "pp. 6848–6856.\n",
      "114376 VOLUME 11, 2023\n",
      "V. Balafas et al.: ML and DL for Plant Disease Classification and Detection\n",
      "[147] S. Zagoruyko and N. Komodakis, ‘‘Wide residual networks,’’ 2016,\n",
      "arXiv:1605.07146.\n",
      "[148] J. Deere. See & Spray Ultimate. Accessed: Sep. 14, 2023. [Online].\n",
      "Available: https://www.deere.com/en/sprayers/see-spray-ultimate/\n",
      "[149] A. Y eshe, P . Gourkhede, and P . V aidya, ‘‘Blue river technology: Futuristic\n",
      "approach of precision farming,’’ Just Agricult., vol. 2, no. 7, pp. 1–14,\n",
      "2022.\n",
      "[150] IBM. Agriculture—Environmental Intelligence Suite.\n",
      "Accessed: Sep. 14, 2023. [Online]. Available: https://www.ibm.com/\n",
      "products/environmental-intelligence-suite/agriculture\n",
      "VASILEIOS BALAFAS received the B.Sc. (inte-\n",
      "grated master’s) degree from the Department of\n",
      "Electrical and Computer Engineering, University\n",
      "of Western Macedonia, Greece, in 2021, where\n",
      "he is currently pursuing the Ph.D. degree, work-\n",
      "ing on developing optimization and constraint\n",
      "programming algorithms for cloud computing\n",
      "optimization. His Diploma thesis focused on com-\n",
      "putationally comparing object detection methods.\n",
      "He has published one journal article and two\n",
      "conference papers.\n",
      "EMMANOUIL KARANTOUMANIS received\n",
      "the B.Sc. (integrated master’s) degree from\n",
      "the Department of Informatics and Telecom-\n",
      "munications Engineering, University of Western\n",
      "Macedonia, Greece, in 2019. He is currently\n",
      "pursuing the Ph.D. degree with the Department of\n",
      "Electrical and Computer Engineering, University\n",
      "of Western Macedonia, working on developing\n",
      "hybrid optimization and machine learning algo-\n",
      "rithms in order to optimize black-box problems.\n",
      "His Diploma thesis focused on applying machine learning algorithms in\n",
      "order to predict power consumption in data centers. He has published five\n",
      "conference papers.\n",
      "MALAMATI LOUTA (Senior Member, IEEE)\n",
      "received the M.Eng. and Ph.D. degrees in electrical\n",
      "and computer engineering, in 1997 and 2000,\n",
      "respectively, and the M.B.A. degree from the\n",
      "National Technical University of Athens, in 2004.\n",
      "She is currently the Director of the Telecom-\n",
      "munication Networks and Advanced Services\n",
      "Laboratory and an Associate Professor with the\n",
      "Electrical and Computer Engineering Department,\n",
      "School of Engineering, University of Western\n",
      "Macedonia, Greece. Her research interests include telecommunication\n",
      "networks and advanced services engineering. She is a member of the ACM\n",
      "and the Technical Chamber of Greece. She serves as an associate editor, the\n",
      "general chair, the technical program committee chair, a member, a session\n",
      "organizer, and a reviewer for a number of international conferences and\n",
      "journals.\n",
      "NIKOLAOS PLOSKAS received the B.Sc., M.Sc.,\n",
      "and Ph.D. degrees in applied informatics from the\n",
      "University of Macedonia, Thessaloniki, Greece,\n",
      "in 2007, 2009, and 2014, respectively. He is\n",
      "currently an Associate Professor with the Depart-\n",
      "ment of Electrical and Computer Engineering,\n",
      "University of Western Macedonia. His research\n",
      "interests include mathematical programming, opti-\n",
      "mization, operations research, analysis and design\n",
      "of algorithms, cloud computing, and machine\n",
      "learning. He is also serving on the editorial board for four scientific journals.\n",
      "He has served on the organizing/program committee for 20 international\n",
      "conferences.\n",
      "VOLUME 11, 2023 114377\n"
     ]
    }
   ],
   "source": [
    "final_text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature1:** Research based Question Answering\n",
    "**Using Output Parser for structured output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T19:10:45.775877Z",
     "iopub.status.busy": "2026-01-23T19:10:45.775132Z",
     "iopub.status.idle": "2026-01-23T19:10:45.780527Z",
     "shell.execute_reply": "2026-01-23T19:10:45.779656Z",
     "shell.execute_reply.started": "2026-01-23T19:10:45.775845Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##to prevent any hallucination in the answer of the question, we removed (temprature, top_k, top_p...)\n",
    "def generate_answer(prompt, max_new_tokens=512):    #max_new to control output independent from input\n",
    "    inputs = tokenizer_QA(prompt,return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,      \n",
    "        num_beams=1,          \n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer_QA.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T19:10:47.238904Z",
     "iopub.status.busy": "2026-01-23T19:10:47.238582Z",
     "iopub.status.idle": "2026-01-23T19:10:47.243480Z",
     "shell.execute_reply": "2026-01-23T19:10:47.242714Z",
     "shell.execute_reply.started": "2026-01-23T19:10:47.238877Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Question_schema = ResponseSchema(\n",
    "    name = \"Question\",\n",
    "    description = \"The question took from user input.\"\n",
    ")\n",
    "\n",
    "Answer_schema = ResponseSchema(\n",
    "    name = \"Answer\",\n",
    "    description = \"the Generated answer from the model.\"\n",
    ")\n",
    "\n",
    "Evidence_schema = ResponseSchema(\n",
    "    name = \"Evidence\",\n",
    "    description = \"list of string containing provided 1-3 exact quotes from the context as evidence, if non, return empty list.\"\n",
    ")\n",
    "\n",
    "Pages_schema = ResponseSchema(\n",
    "    name = \"Pages\",\n",
    "    description = \"list of integers that refere to page number containing the answer of user question.\"\n",
    ")\n",
    "\n",
    "response_schema = [Question_schema,Answer_schema,Evidence_schema,Pages_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T19:10:48.815424Z",
     "iopub.status.busy": "2026-01-23T19:10:48.814855Z",
     "iopub.status.idle": "2026-01-23T19:10:48.819054Z",
     "shell.execute_reply": "2026-01-23T19:10:48.818291Z",
     "shell.execute_reply.started": "2026-01-23T19:10:48.815396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T19:10:49.734984Z",
     "iopub.status.busy": "2026-01-23T19:10:49.734230Z",
     "iopub.status.idle": "2026-01-23T19:10:49.738680Z",
     "shell.execute_reply": "2026-01-23T19:10:49.737897Z",
     "shell.execute_reply.started": "2026-01-23T19:10:49.734953Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"Question\": string  // The question took from user input.\n",
      "\t\"Answer\": string  // the Generated answer from the model.\n",
      "\t\"Evidence\": string  // list of string containing provided 1-3 exact quotes from the context as evidence, if non, return empty list.\n",
      "\t\"Pages\": string  // list of integers that refere to page number containing the answer of user question.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T19:10:51.015976Z",
     "iopub.status.busy": "2026-01-23T19:10:51.015696Z",
     "iopub.status.idle": "2026-01-23T19:10:51.019761Z",
     "shell.execute_reply": "2026-01-23T19:10:51.019155Z",
     "shell.execute_reply.started": "2026-01-23T19:10:51.015951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "purchase_extraction_template = \"\"\"\n",
    "You must answer using ONLY the context below.\n",
    "Rules:\n",
    "- Do NOT use outside knowledge.\n",
    "- Do NOT explain beyond what the context says.\n",
    "- If the answer is not explicitly stated in the context, set \"answer\" to \"Not mentioned\" and \"evidence\" to [].\n",
    "- If the answer is a list: list ALL items explicitly.\n",
    "- After the answer, provide 1-3 exact quotes from the context as evidence.\n",
    "\n",
    "Respond ONLY in JSON format as follows:\n",
    "{format_instructions}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Now answer the following question:\n",
    "\"{query}\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T19:46:32.231523Z",
     "iopub.status.busy": "2026-01-23T19:46:32.231048Z",
     "iopub.status.idle": "2026-01-23T19:46:32.236849Z",
     "shell.execute_reply": "2026-01-23T19:46:32.236067Z",
     "shell.execute_reply.started": "2026-01-23T19:46:32.231495Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def Question_Answering(query):\n",
    "    docs = vectordb.similarity_search(query, k=6)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=purchase_extraction_template,\n",
    "        input_variables=[\"context\",\"query\",\"format_instructions\"]\n",
    "    ).format(context =context ,query =query , format_instructions=format_instructions)\n",
    "    \n",
    "    answer = generate_answer(prompt, max_new_tokens=512)\n",
    "\n",
    "    pattern = r'```json\\s*(.*?)\\s*```'\n",
    "    matches = re.findall(pattern, answer, re.DOTALL)\n",
    "\n",
    "    json_output = f\"```json\\n{matches[-1]}\\n```\" if matches else None\n",
    "\n",
    "    ##set to prevent repitition in the pages\n",
    "    pages = sorted(set(\n",
    "        [doc.metadata.get('page', 'Unknown') for doc in docs]\n",
    "    ))\n",
    "    return json_output.strip(), pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T19:10:53.554712Z",
     "iopub.status.busy": "2026-01-23T19:10:53.554395Z",
     "iopub.status.idle": "2026-01-23T19:11:12.671603Z",
     "shell.execute_reply": "2026-01-23T19:11:12.670737Z",
     "shell.execute_reply.started": "2026-01-23T19:10:53.554683Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: ```json\n",
      "{\n",
      "\t\"Question\": \"According to the paper’s computational study on the PlantDoc dataset, which object detection model achieved the highest accuracy, and which classification networks had the best trade-off between accuracy and training time?\",\n",
      "\t\"Answer\": \"The object detection model that achieved the highest accuracy was YOLOv5. For the image classification task, the networks ResNet50 and MobileNetv2 had the most optimal trade-off on accuracy and training time.\",\n",
      "\t\"Evidence\": [\"For the object detection problem, computational results show that object detection accuracy is high with YOLOv5.\", \"For the image classification task, the networks ResNet50 and MobileNetv2 have the most optimal trade-off on accuracy and training time.\"],\n",
      "\t\"Pages\": [1, 2]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "Question = 'According to the paper’s computational study on the PlantDoc dataset, which object detection model achieved the highest accuracy, and which classification networks had the best trade-off between accuracy and training time?'\n",
    "result, page_number = Question_Answering(Question)\n",
    "print(\"answer:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q/A Deplyment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T19:12:13.736144Z",
     "iopub.status.busy": "2026-01-23T19:12:13.735516Z",
     "iopub.status.idle": "2026-01-23T19:12:13.739791Z",
     "shell.execute_reply": "2026-01-23T19:12:13.739035Z",
     "shell.execute_reply.started": "2026-01-23T19:12:13.736115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NGROK_TOKEN = \"386REcz4JsU2OpwGtAeOqsd8H7j_81puHo7ELtjbx6QxVhpZp\"\n",
    "API_KEY = \"secret123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T19:12:15.257390Z",
     "iopub.status.busy": "2026-01-23T19:12:15.256663Z",
     "iopub.status.idle": "2026-01-23T19:12:15.261137Z",
     "shell.execute_reply": "2026-01-23T19:12:15.260430Z",
     "shell.execute_reply.started": "2026-01-23T19:12:15.257360Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_text_from_bytes(pdf_bytes):\n",
    "    pdf_file = io.BytesIO(pdf_bytes)\n",
    "    reader = PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T19:48:19.912608Z",
     "iopub.status.busy": "2026-01-23T19:48:19.912020Z",
     "iopub.status.idle": "2026-01-23T19:48:19.916308Z",
     "shell.execute_reply": "2026-01-23T19:48:19.915585Z",
     "shell.execute_reply.started": "2026-01-23T19:48:19.912577Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_json_block(text):\n",
    "    pattern = r'```json\\s*(.*?)\\s*```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches[-1].strip() if matches else text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature3:** Basic Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T20:32:29.349395Z",
     "iopub.status.busy": "2026-01-23T20:32:29.348908Z",
     "iopub.status.idle": "2026-01-23T20:32:29.353448Z",
     "shell.execute_reply": "2026-01-23T20:32:29.352715Z",
     "shell.execute_reply.started": "2026-01-23T20:32:29.349362Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_final_answer(text: str) -> str:\n",
    "    if \"Answer:\" in text:\n",
    "        return text.split(\"Answer:\")[-1].strip()\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T20:32:30.065597Z",
     "iopub.status.busy": "2026-01-23T20:32:30.065302Z",
     "iopub.status.idle": "2026-01-23T20:32:30.070361Z",
     "shell.execute_reply": "2026-01-23T20:32:30.069371Z",
     "shell.execute_reply.started": "2026-01-23T20:32:30.065570Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def Question_Answering_chain(query):\n",
    "    docs = vectordb.similarity_search(query, k=6)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "            Answer ONLY using the context below.\n",
    "            Return ONLY the final answer.\n",
    "            Do NOT explain.\n",
    "            Do NOT repeat the context.\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question:\n",
    "            {query}\n",
    "            \"\"\"\n",
    "\n",
    "    answer = generate_answer(prompt, max_new_tokens=512)\n",
    "    return extract_final_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T20:32:31.697232Z",
     "iopub.status.busy": "2026-01-23T20:32:31.696957Z",
     "iopub.status.idle": "2026-01-23T20:32:31.706434Z",
     "shell.execute_reply": "2026-01-23T20:32:31.705763Z",
     "shell.execute_reply.started": "2026-01-23T20:32:31.697207Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##make open source as closed source\n",
    "class CustomHFLLM(LLM):\n",
    "    def _call(self, prompt: str, stop: Any = None) -> str:\n",
    "        return Question_Answering_chain(prompt)\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom_huggingface\"\n",
    "\n",
    "llm = CustomHFLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T20:32:33.460536Z",
     "iopub.status.busy": "2026-01-23T20:32:33.460250Z",
     "iopub.status.idle": "2026-01-23T20:32:33.466756Z",
     "shell.execute_reply": "2026-01-23T20:32:33.466007Z",
     "shell.execute_reply.started": "2026-01-23T20:32:33.460509Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/3364785747.py:6: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  problem_chain = LLMChain(llm=llm, prompt=problem_prompt)\n"
     ]
    }
   ],
   "source": [
    "# Chain 1: problem statement\n",
    "problem_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Extract from this paper the problem statement,Return ONLY the main research problem in 2-3 sentences\"\n",
    ")\n",
    "problem_chain = LLMChain(llm=llm, prompt=problem_prompt)\n",
    "\n",
    "# Chain 2: Datasets\n",
    "dataset_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template='''List all the datasets used in this paper, for each dataset mention its name, \n",
    "                number of images and the classes it include.\n",
    "                Return ONLY JSON format:\n",
    "                [{{\"name\": \"PlantDoc\", \"images\": 5000, \"classes\": [\"disease1\", \"disease2\"]}}]'''\n",
    ")\n",
    "datasete_chain = LLMChain(llm=llm, prompt=dataset_prompt)\n",
    "\n",
    "# Chain 3: Models\n",
    "model_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template='''List ALL ML/DL MODELS used in this paper.\n",
    "                Include: CNNs, object detectors, classifiers.\n",
    "                Return ONLY JSON: [{{\"name\": \"ResNet50\", \"type\": \"CNN\", \"task\": \"classification\"}}]'''\n",
    ")\n",
    "model_chain = LLMChain(llm=llm, prompt=model_prompt)\n",
    "\n",
    "# Chain 4: Results\n",
    "result_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=(\"List the metrics used to evaluate the model performance and list the final results of the work provided in the paper.\")\n",
    ")\n",
    "result_chain = LLMChain(llm=llm, prompt=result_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T06:34:52.272184Z",
     "iopub.status.busy": "2026-01-21T06:34:52.271424Z",
     "iopub.status.idle": "2026-01-21T06:34:52.276292Z",
     "shell.execute_reply": "2026-01-21T06:34:52.275695Z",
     "shell.execute_reply.started": "2026-01-21T06:34:52.272154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_all():\n",
    "    problem = problem_chain.run(question= \"What is the main research problem?\")\n",
    "    print(f\"\\nProblem Statement:\\n{problem}\")\n",
    "\n",
    "    dataset = datasete_chain.run(question= \"What datasets are used in the paper?\")\n",
    "    print(f\"\\nDatasets:\\n{dataset}\")\n",
    "\n",
    "    models = model_chain.run(question = \"What models are used in the paper?\")\n",
    "    print(f\"\\nModels:\\n{models}\")\n",
    "\n",
    "    results = result_chain.run(question= \"What are the evaluation metrics and final results?\")\n",
    "    print(f\"\\nFinal Results:\\n{results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T06:34:53.447953Z",
     "iopub.status.busy": "2026-01-21T06:34:53.447670Z",
     "iopub.status.idle": "2026-01-21T06:36:11.282073Z",
     "shell.execute_reply": "2026-01-21T06:36:11.281317Z",
     "shell.execute_reply.started": "2026-01-21T06:34:53.447929Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem Statement:\n",
      "The main research problem is the development of machine learning algorithms for plant disease detection and classification, focusing on distinguishing between healthy plants and those with diseases such as common rust, Northern blight, and multiple diseases in a single image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets:\n",
      "[\n",
      "                {\"name\": \"PlantLeaves\", \"images\": 4503, \"classes\": 22},\n",
      "                {\"name\": \"PlantDoc\", \"images\": 5000, \"classes\": 15},\n",
      "                {\"name\": \"PlantVillage\", \"images\": 87848, \"classes\": 58}\n",
      "            ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models:\n",
      "[\n",
      "                {\"name\": \"ResNet50\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"VGG16\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"InceptionV3\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"DenseNet121\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"GoogleNet\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"AlexNet\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"DenseNet121\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"DenseNet161\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"DenseNet169\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"DenseNet201\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"MobileNetV2\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"ResNet50\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"ResNet101\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"ResNet151\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"ResNeXt50\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"ResNeXt101\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"ShuffleNet\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"VGG\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"WideResNet50\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"WideResNet101\", \"type\": \"CNN\", \"task\": \"classification\"},\n",
      "                {\"name\": \"YOLO\", \"type\": \"object detector\", \"task\": \"detection\"},\n",
      "                {\"name\": \"YOLOv5\", \"\n",
      "\n",
      "Final Results:\n",
      "Metrics used: accuracy, F1-score, precision, recall.\n",
      "            Final result: DenseNet121 model achieved the highest accuracy of 61.01%.\n"
     ]
    }
   ],
   "source": [
    "run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T20:32:48.845034Z",
     "iopub.status.busy": "2026-01-23T20:32:48.844372Z",
     "iopub.status.idle": "2026-01-23T20:32:48.849626Z",
     "shell.execute_reply": "2026-01-23T20:32:48.848942Z",
     "shell.execute_reply.started": "2026-01-23T20:32:48.844995Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_all_api():\n",
    "    problem = problem_chain.run(question=\"What is the main research problem?\")\n",
    "\n",
    "    dataset = datasete_chain.run(question=\"What datasets are used in the paper?\")\n",
    "\n",
    "    models = model_chain.run(question=\"What models are used in the paper?\")\n",
    "\n",
    "    results = result_chain.run(question=\"What are the evaluation metrics and final results?\")\n",
    "    return {\n",
    "        \"problem_statement\": problem,\n",
    "        \"datasets\": dataset,\n",
    "        \"models\": models,\n",
    "        \"results\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T21:36:59.149688Z",
     "iopub.status.busy": "2026-01-23T21:36:59.149074Z",
     "iopub.status.idle": "2026-01-23T21:36:59.156785Z",
     "shell.execute_reply": "2026-01-23T21:36:59.155959Z",
     "shell.execute_reply.started": "2026-01-23T21:36:59.149650Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/QA\")\n",
    "async def qa_endpoint(req: Request):\n",
    "    if req.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n",
    "    data = await req.json()\n",
    "    pdf = data.get(\"pdf_file\")\n",
    "    pdf_bytes = base64.b64decode(pdf)\n",
    "    pdf_text = extract_text_from_bytes(pdf_bytes)\n",
    "    chunks, embedding, vectordb = prepare_pdf(pdf_text)\n",
    "    question = data.get(\"question\")\n",
    "    raw_json, pages = Question_Answering(question)\n",
    "    json_text = extract_json_block(raw_json)\n",
    "    output_data = output_parser.parse(json_text)\n",
    "    return {\"answer\": {**output_data, \"Pages\": pages}}\n",
    "\n",
    "# --- Info Extractor Endpoint ---\n",
    "@app.post(\"/info_Extractor\")\n",
    "async def extractor_endpoint(req: Request):\n",
    "    if req.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n",
    "    data = await req.json()\n",
    "    pdf = data.get(\"pdf_file\")\n",
    "    pdf_bytes = base64.b64decode(pdf)\n",
    "    pdf_text = extract_text_from_bytes(pdf_bytes)\n",
    "    prepare_pdf(pdf_text)\n",
    "    output = run_all_api()\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T21:37:07.159609Z",
     "iopub.status.busy": "2026-01-23T21:37:07.159001Z",
     "iopub.status.idle": "2026-01-23T21:37:08.471286Z",
     "shell.execute_reply": "2026-01-23T21:37:08.470522Z",
     "shell.execute_reply.started": "2026-01-23T21:37:07.159580Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your public URL: https://pennie-sabulous-rheba.ngrok-free.dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [55]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:60507 (Press CTRL+C to quit)\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     41.199.138.164:0 - \"POST /QA HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     41.199.138.164:0 - \"POST /info_Extractor HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     41.199.138.164:0 - \"POST /QA HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "def free_port():\n",
    "    s = socket.socket()\n",
    "    s.bind(('', 0))\n",
    "    port = s.getsockname()[1]\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "port = free_port()\n",
    "conf.get_default().auth_token = NGROK_TOKEN\n",
    "public_url = ngrok.connect(port).public_url\n",
    "print(\"Your public URL:\", public_url)\n",
    "\n",
    "def run(): uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
    "threading.Thread(target=run, daemon=True).start()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9280930,
     "sourceId": 14530952,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
